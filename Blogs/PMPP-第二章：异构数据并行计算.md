---
title: PMPP-ç¬¬äºŒç« ï¼šå¼‚æ„æ•°æ®å¹¶è¡Œè®¡ç®—
date: 2026-01-11 19:38:29
tags:
  - CUDA
  - GPUç¼–ç¨‹
  - å¹¶è¡Œè®¡ç®—
  - PMPP
  - å‘é‡åŠ æ³•
categories: çŸ¥è¯†åˆ†äº«
cover: /img/PMPP.jpg
---

## å‰è¨€

ç¬¬ä¸€ç« è®²äº†ç†è®ºï¼Œç¬¬äºŒç« å¼€å§‹å†™ä»£ç äº†ã€‚è™½ç„¶ä¾‹å­æ˜¯ç»å…¸çš„å‘é‡åŠ æ³•ï¼Œä½†å®ƒåŒ…å«äº†CUDAç¼–ç¨‹çš„æ‰€æœ‰æ ¸å¿ƒç¯èŠ‚ï¼šå†…å­˜ç®¡ç†ã€kernelç¼–å†™ã€çº¿ç¨‹ç»„ç»‡ã€‚æŒæ¡è¿™ä¸ªç®€å•ä¾‹å­ï¼Œåé¢çš„å¤æ‚åº”ç”¨å°±æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šçš„æ‰©å±•ã€‚

> **ğŸ“¦ é…å¥—èµ„æº**ï¼šæœ¬ç³»åˆ—æ–‡ç« é…æœ‰å®Œæ•´çš„ [GitHub ä»“åº“](https://github.com/psmarter/PMPP-Learning)ï¼ŒåŒ…å«æ¯ç« çš„ç»ƒä¹ é¢˜è§£ç­”ã€CUDA ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚æ‰€æœ‰ä»£ç éƒ½ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œã€‚

## ä¸ºä»€ä¹ˆä»å‘é‡åŠ æ³•å¼€å§‹

### æ•°æ®å¹¶è¡Œçš„å…¸å‹ä¾‹å­

å‘é‡åŠ æ³•æ˜¯æ•°æ®å¹¶è¡Œçš„æœ€ä½³å…¥å£ï¼š

```
C[0] = A[0] + B[0]
C[1] = A[1] + B[1]
...
C[n-1] = A[n-1] + B[n-1]
```

æ¯ä¸ªå…ƒç´ çš„è®¡ç®—å®Œå…¨ç‹¬ç«‹ï¼ŒC[0]ä¸éœ€è¦ç­‰C[1]ç®—å®Œã€‚è¿™ç§ç‹¬ç«‹æ€§æ­£æ˜¯å¹¶è¡Œè®¡ç®—çš„é»„é‡‘åœºæ™¯ã€‚

### å†…å­˜å—é™é—®é¢˜

å‘é‡åŠ æ³•çš„ç®—æœ¯å¼ºåº¦å¾ˆä½ï¼š

- æ¯å…ƒç´ ï¼šè¯»2ä¸ªfloat + å†™1ä¸ªfloat = 12å­—èŠ‚
- è®¡ç®—ï¼š1æ¬¡æµ®ç‚¹åŠ æ³•
- ç®—æœ¯å¼ºåº¦ï¼š1 FLOP / 12 Bytes â‰ˆ 0.083 FLOP/Byte

å…¸å‹å†…å­˜å—é™ï¼ˆMemory-Boundï¼‰é—®é¢˜ã€‚GPUè®¡ç®—å•å…ƒä¼šç»å¸¸ç­‰æ•°æ®ã€‚è™½ç„¶æ€§èƒ½è¾¾ä¸åˆ°å³°å€¼ï¼Œä½†ä½œä¸ºå…¥é—¨ä¾‹å­è¶³å¤Ÿç®€å•ç›´è§‚ã€‚

## CUDAç¨‹åºç»“æ„ï¼šä¸‰æ­¥èµ°

### CPUç‰ˆæœ¬ï¼ˆå¯¹æ¯”ï¼‰

```c
void vecAddCPU(float *A, float *B, float *C, int n) {
    for (int i = 0; i < n; i++) {
        C[i] = A[i] + B[i];
    }
}
```

ä¸²è¡Œæ‰§è¡Œï¼Œn=10000å°±è¦å¾ªç¯10000æ¬¡ã€‚

### CUDAç‰ˆæœ¬

#### 1. Hostç«¯å‡†å¤‡

```cuda
int n = 10000;
size_t size = n * sizeof(float);

// åˆ†é…Hostå†…å­˜
float *h_A = (float*)malloc(size);
float *h_B = (float*)malloc(size);
float *h_C = (float*)malloc(size);

// åˆå§‹åŒ–æ•°æ®
for (int i = 0; i < n; i++) {
    h_A[i] = i * 1.0f;
    h_B[i] = i * 2.0f;
}
```

`h_`å‰ç¼€è¡¨ç¤ºHostå˜é‡ï¼Œè¿™æ˜¯ä¸ªå¥½ä¹ æƒ¯ã€‚

#### 2. Deviceç«¯å‡†å¤‡

```cuda
// åˆ†é…Deviceå†…å­˜
float *d_A, *d_B, *d_C;
cudaMalloc((void**)&d_A, size);
cudaMalloc((void**)&d_B, size);
cudaMalloc((void**)&d_C, size);

// Host â†’ Device
cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
```

**å…³é”®**ï¼š

- `cudaMalloc`å‚æ•°æ˜¯äºŒçº§æŒ‡é’ˆï¼ˆéœ€è¦ä¿®æ”¹æŒ‡é’ˆå€¼ï¼‰
- `d_A`æ˜¯DeviceæŒ‡é’ˆï¼Œåœ¨Hostä»£ç ä¸­ä¸èƒ½ç›´æ¥è§£å¼•ç”¨`d_A[0]`ï¼ˆä¼šæ®µé”™è¯¯ï¼‰

#### 3. æ‰§è¡Œä¸å›ä¼ 

```cuda
// å¯åŠ¨kernel
int threadsPerBlock = 256;
int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
vecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);

// Device â†’ Host
cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

// éªŒè¯
for (int i = 0; i < n; i++) {
    if (fabs(h_C[i] - (h_A[i] + h_B[i])) > 1e-5) {
        printf("Error at index %d\n", i);
    }
}

// æ¸…ç†
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
free(h_A);
free(h_B);
free(h_C);
```

## Kernelå‡½æ•°

### åŸºæœ¬ç»“æ„

```cuda
__global__ void vecAdd(float *A, float *B, float *C, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}
```

**é€è¡Œè§£æ**ï¼š

- `__global__`ï¼šGPUä¸Šæ‰§è¡Œï¼ŒCPUè°ƒç”¨
  - `__device__`ï¼šGPUä¸Šæ‰§è¡Œï¼ŒGPUè°ƒç”¨
  - `__host__`ï¼šCPUä¸Šæ‰§è¡Œï¼ŒCPUè°ƒç”¨ï¼ˆé»˜è®¤ï¼Œå¯çœç•¥ï¼‰

- çº¿ç¨‹ç´¢å¼•è®¡ç®—ï¼š`i = blockIdx.x * blockDim.x + threadIdx.x`
  - `blockIdx.x`ï¼šblockåœ¨gridä¸­çš„ç´¢å¼•
  - `blockDim.x`ï¼šblockçš„å¤§å°
  - `threadIdx.x`ï¼šthreadåœ¨blockä¸­çš„ç´¢å¼•

- è¾¹ç•Œæ£€æŸ¥ï¼š`if (i < n)` å¿…é¡»æœ‰ï¼ˆæ€»çº¿ç¨‹æ•°é€šå¸¸å¤šäºæ•°ç»„å…ƒç´ ï¼‰

### çº¿ç¨‹å±‚æ¬¡ç»“æ„

```
Grid
â”œâ”€â”€ Block 0 (256 threads)
â”‚   â”œâ”€â”€ Thread 0   â†’ i = 0*256 + 0 = 0
â”‚   â”œâ”€â”€ Thread 1   â†’ i = 0*256 + 1 = 1
â”‚   â””â”€â”€ Thread 255 â†’ i = 0*256 + 255 = 255
â”œâ”€â”€ Block 1 (256 threads)
â”‚   â”œâ”€â”€ Thread 0   â†’ i = 1*256 + 0 = 256
â”‚   â””â”€â”€ Thread 255 â†’ i = 1*256 + 255 = 511
â””â”€â”€ ...
```

æ¯ä¸ªthreadå¾—åˆ°å”¯ä¸€ç´¢å¼•ï¼Œå¯¹åº”æ•°ç»„å…ƒç´ ã€‚

### ä¸ºä»€ä¹ˆ256ä¸ªthreadsï¼Ÿ

ä¸æ˜¯éšä¾¿é€‰çš„ï¼š

1. **Warpçš„å€æ•°**ï¼šGPUä»¥32çº¿ç¨‹ä¸ºä¸€ç»„ï¼ˆwarpï¼‰æ‰§è¡Œï¼Œ256 = 8 Ã— 32
2. **ç¡¬ä»¶é™åˆ¶**ï¼šæ¯blockæœ€å¤š1024 threads
3. **ç»éªŒå€¼**ï¼š128-512é€šå¸¸æ€§èƒ½è¾ƒå¥½

å…·ä½“æœ€ä¼˜å€¼éœ€è¦profilingç¡®å®šã€‚

### è¾¹ç•Œæ£€æŸ¥çš„å¿…è¦æ€§

```cuda
blocksPerGrid = (10000 + 255) / 256 = 40
æ€»threads = 40 Ã— 256 = 10240
```

å¤šå‡º240ä¸ªçº¿ç¨‹ã€‚ä¸æ£€æŸ¥è¾¹ç•Œä¼šè¶Šç•Œè®¿é—®ï¼Œå¯¼è‡´é”™è¯¯æˆ–å´©æºƒã€‚

## å†…å­˜ç®¡ç†

### Host vs Deviceå†…å­˜

**å…³é”®**ï¼šä¸¤ä¸ªç‹¬ç«‹çš„å†…å­˜ç©ºé—´ï¼Œä¸èƒ½ç›´æ¥äº’è®¿ã€‚

- **Host Memory**ï¼šCPUçš„DDR4/DDR5
- **Device Memory**ï¼šGPUçš„GDDR6/HBM

**é”™è¯¯ç¤ºä¾‹**ï¼š

```cuda
float *d_A;
cudaMalloc((void**)&d_A, size);
d_A[0] = 1.0f;  // æ®µé”™è¯¯ï¼CPUä¸èƒ½ç›´æ¥è®¿é—®GPUå†…å­˜
```

**æ­£ç¡®åšæ³•**ï¼š

```cuda
float *h_A = (float*)malloc(size);
h_A[0] = 1.0f;
cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
```

### æ•°æ®ä¼ è¾“å¼€é”€

PCIeå¸¦å®½ï¼ˆ~32 GB/sï¼‰è¿œä½äºGPUå†…å­˜å¸¦å®½ï¼ˆ500+ GB/sï¼‰ã€‚å¯¹äºç®€å•è®¡ç®—ï¼Œä¼ è¾“æ—¶é—´å¯èƒ½æ˜¯è®¡ç®—æ—¶é—´çš„æ•°åå€ã€‚

**ä¼˜åŒ–åŸåˆ™**ï¼š

- å‡å°‘ä¼ è¾“æ¬¡æ•°ï¼ˆæ‰¹é‡ä¼ è¾“ï¼‰
- ä¿æŒæ•°æ®åœ¨GPUï¼ˆå¤šæ­¥è®¡ç®—ä¸å›ä¼ ï¼‰
- å¼‚æ­¥ä¼ è¾“ä¸è®¡ç®—é‡å ï¼ˆé«˜çº§æŠ€å·§ï¼‰

### Unified Memoryï¼ˆå¯é€‰ï¼‰

ä»CUDA 6.0èµ·å¯ä»¥ç”¨ï¼š

```cuda
float *data;
cudaMallocManaged(&data, size);

data[0] = 1.0f;              // CPUè®¿é—®
kernel<<<...>>>(data);       // GPUè®¿é—®ï¼ˆè‡ªåŠ¨è¿ç§»ï¼‰
printf("%f\n", data[0]);     // CPUè®¿é—®ï¼ˆè‡ªåŠ¨ä¼ å›ï¼‰

cudaFree(data);
```

æ–¹ä¾¿ï¼Œä½†æœ‰æ€§èƒ½å¼€é”€ã€‚å­¦ä¹ åŸå‹å¼€å‘å‹å¥½ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®æ˜¾å¼ç®¡ç†ã€‚

## æ‰§è¡Œé…ç½®

### å¯åŠ¨è¯­æ³•

```cuda
vecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, n);
```

å®Œæ•´å½¢å¼ï¼š

```cuda
kernel<<<gridDim, blockDim, sharedMem, stream>>>(args);
```

- `gridDim`ï¼šgridçš„ç»´åº¦ï¼ˆ1D/2D/3Dï¼‰
- `blockDim`ï¼šblockçš„ç»´åº¦
- `sharedMem`ï¼šå…±äº«å†…å­˜å¤§å°ï¼ˆå¯é€‰ï¼Œé»˜è®¤0ï¼‰
- `stream`ï¼šCUDAæµï¼ˆå¯é€‰ï¼Œé»˜è®¤0ï¼‰

### è®¡ç®—gridå¤§å°

```cuda
int threads = 256;
int blocks = (n + threads - 1) / threads;  // å‘ä¸Šå–æ•´
```

æ•°å­¦ç­‰ä»·äº`ceil(n / threads)`ï¼Œä½†æ•´æ•°è¿ç®—æ›´é«˜æ•ˆã€‚

## é”™è¯¯å¤„ç†

CUDAå‡½æ•°è¿”å›`cudaError_t`ï¼Œéœ€è¦æ˜¾å¼æ£€æŸ¥ï¼š

```cuda
#define CUDA_CHECK(call) \
do { \
    cudaError_t err = call; \
    if (err != cudaSuccess) { \
        fprintf(stderr, "CUDA Error: %s at %s:%d\n", \
                cudaGetErrorString(err), __FILE__, __LINE__); \
        exit(EXIT_FAILURE); \
    } \
} while(0)

// ä½¿ç”¨
CUDA_CHECK(cudaMalloc((void**)&d_A, size));
CUDA_CHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
```

Kernelå¯åŠ¨ä¸è¿”å›é”™è¯¯ç ï¼Œéœ€è¦ï¼š

```cuda
vecAdd<<<blocks, threads>>>(d_A, d_B, d_C, n);
CUDA_CHECK(cudaGetLastError());          // æ£€æŸ¥å¯åŠ¨é”™è¯¯
CUDA_CHECK(cudaDeviceSynchronize());     // åŒæ­¥å¹¶æ£€æŸ¥æ‰§è¡Œé”™è¯¯
```

## å°ç»“

ç¬¬äºŒç« é€šè¿‡å‘é‡åŠ æ³•å»ºç«‹äº†CUDAç¼–ç¨‹çš„åŸºæœ¬æ¡†æ¶ï¼š

**æ ¸å¿ƒæµç¨‹**ï¼šå†…å­˜åˆ†é… â†’ æ•°æ®ä¼ è¾“ â†’ kernelå¯åŠ¨ â†’ ç»“æœå›ä¼ ï¼Œè¿™æ˜¯æ‰€æœ‰CUDAç¨‹åºçš„éª¨æ¶ã€‚

**çº¿ç¨‹ç»„ç»‡**ï¼šGrid/Block/Threadä¸‰çº§ç»“æ„ï¼Œç´¢å¼•è®¡ç®—`i = blockIdx.x * blockDim.x + threadIdx.x`è¦çƒ‚ç†Ÿäºå¿ƒã€‚

**å†…å­˜æ¨¡å‹**ï¼šHostå’ŒDeviceæ˜¯ç‹¬ç«‹ç©ºé—´ï¼Œå¿…é¡»æ˜¾å¼ä¼ è¾“ã€‚æ•°æ®ä¼ è¾“å¼€é”€ä¸å®¹å¿½è§†ã€‚

**æ€§èƒ½è®¤çŸ¥**ï¼šå‘é‡åŠ æ³•è™½ç„¶èƒ½åœ¨GPUä¸Šè·‘ï¼Œä½†å—å†…å­˜å¸¦å®½é™åˆ¶ï¼Œæ€§èƒ½æå‡æœ‰é™ã€‚çœŸæ­£å‘æŒ¥GPUä¼˜åŠ¿éœ€è¦é«˜ç®—æœ¯å¼ºåº¦çš„ä»»åŠ¡ã€‚

**ä»£ç ä¹ æƒ¯**ï¼š

- å˜é‡å‘½ååŒºåˆ†h_/d_
- è¾¹ç•Œæ£€æŸ¥å¿…é¡»ä¸¥æ ¼
- é”™è¯¯å¤„ç†ä¸èƒ½çœç•¥

ä¸‹ä¸€ç« è¿›å…¥å¤šç»´æ•°æ®å¤„ç†ï¼ˆçŸ©é˜µã€å›¾åƒï¼‰ï¼Œä¼šç”¨åˆ°2D Grid/Blockç»„ç»‡ã€‚ç†è§£äº†ä¸€ç»´çš„åŸç†ï¼Œå¤šç»´åªæ˜¯è‡ªç„¶æ‰©å±•ã€‚

---

**å‚è€ƒèµ„æ–™ï¼š**

- Hwu, W., Kirk, D., & El Hajj, I. (2022). *Programming Massively Parallel Processors: A Hands-on Approach* (4th Edition). Morgan Kaufmann.
- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

---

> **æœ¬æ–‡ GitHub ä»“åº“**: [https://github.com/psmarter/PMPP-Learning](https://github.com/psmarter/PMPP-Learning)
