---
title: PMPP-ç¬¬ä¸‰ç« ï¼šå¤šç»´ç½‘æ ¼å’Œæ•°æ®
tags:
  - CUDA
  - GPUç¼–ç¨‹
  - å¹¶è¡Œè®¡ç®—
  - PMPP
  - çŸ©é˜µä¹˜æ³•
  - å›¾åƒå¤„ç†
categories: çŸ¥è¯†åˆ†äº«
cover: /img/PMPP.jpg
abbrlink: 6b7045b6
date: 2026-01-12 15:19:16
---

## å‰è¨€

ç¬¬äºŒç« çš„å‘é‡åŠ æ³•æ˜¯ä¸€ç»´æ•°æ®ï¼Œå®é™…åº”ç”¨å¤§å¤šæ˜¯å¤šç»´çš„â€”â€”å›¾åƒæ˜¯2Dï¼ŒçŸ©é˜µæ˜¯2Dï¼Œæ·±åº¦å­¦ä¹ å¼ é‡æ˜¯3D/4Dã€‚ç¬¬ä¸‰ç« è®²è§£å¦‚ä½•ç”¨CUDAçš„å¤šç»´Grid/Blockå¤„ç†è¿™äº›æ•°æ®ï¼Œæ ¸å¿ƒæ¡ˆä¾‹æ˜¯çŸ©é˜µä¹˜æ³•å’Œå›¾åƒå¤„ç†ã€‚è™½ç„¶è¿˜æ˜¯åŸºç¡€å®ç°ï¼Œä½†å·²ç»èƒ½çœ‹å‡ºGPUç¼–ç¨‹çš„æ€ç»´å’Œä¼˜åŒ–æ–¹å‘ã€‚

> **ğŸ“¦ é…å¥—èµ„æº**ï¼šæœ¬ç³»åˆ—æ–‡ç« é…æœ‰å®Œæ•´çš„ [GitHub ä»“åº“](https://github.com/psmarter/PMPP-Learning)ï¼ŒåŒ…å«æ¯ç« çš„ç»ƒä¹ é¢˜è§£ç­”ã€CUDA ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚æ‰€æœ‰ä»£ç éƒ½ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œã€‚

## å¤šç»´çº¿ç¨‹ç»„ç»‡

### ä»ä¸€ç»´åˆ°äºŒç»´

ç¬¬äºŒç« ä½¿ç”¨ä¸€ç»´ç´¢å¼•ï¼š

```cuda
int i = blockIdx.x * blockDim.x + threadIdx.x;
```

å¤„ç†çŸ©é˜µè‹¥ç”¨ä¸€ç»´ï¼Œéœ€è¦è½¬æ¢ï¼š

```cuda
int idx = blockIdx.x * blockDim.x + threadIdx.x;
int row = idx / width;  // é™¤æ³•
int col = idx % width;  // å–æ¨¡
```

é™¤æ³•å’Œå–æ¨¡åœ¨ GPU ä¸Šæœ‰å¼€é”€ï¼Œå¯è¯»æ€§ä¹Ÿå·®ã€‚ä½¿ç”¨äºŒç»´æ›´è‡ªç„¶ï¼š

```cuda
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;
```

### dim3ç±»å‹

```cuda
dim3 blockDim(16, 16);     // 256 threads: 16Ã—16
dim3 gridDim(64, 64);      // 4096 blocks
kernel<<<gridDim, blockDim>>>(args);
```

å¦‚æœç”¨æ•´æ•°ä¼šè‡ªåŠ¨è½¬ä¸º1Dï¼š

```cuda
kernel<<<256, 128>>>(args);  // ç­‰ä»·äº dim3(256,1,1), dim3(128,1,1)
```

### é…ç½®åŸåˆ™

- **åŒ¹é…æ•°æ®ç»´åº¦**ï¼šå›¾åƒç”¨2Dï¼Œä½“æ•°æ®ç”¨3D
- **è€ƒè™‘å†…å­˜è®¿é—®**ï¼šåŒä¸€warpçš„çº¿ç¨‹æœ€å¥½è®¿é—®è¿ç»­å†…å­˜
- **ç¡¬ä»¶é™åˆ¶**ï¼š
  - æ¯blockæœ€å¤š1024 threads
  - Gridçš„y/zç»´æœ€å¤š65535 blocks

### å¸¸è§é”™è¯¯

**1. å‚æ•°é¡ºåº**ï¼š`<<<grid, block>>>`ä¸è¦å

**2. å‘ä¸Šå–æ•´**ï¼š

```cuda
// é”™è¯¯ï¼šsizeæ•´é™¤æ—¶ä¼šå¤šä¸€ä¸ªblock
int blocks = size / threads + 1;

// æ­£ç¡®
int blocks = (size + threads - 1) / threads;
```

**3. è¶…è¿‡é™åˆ¶**ï¼š

```cuda
dim3 block(32, 32, 1);   // 32*32=1024ï¼Œåˆšå¥½
dim3 block(32, 32, 2);   // 2048ï¼Œè¶…é™ï¼
```

## çŸ©é˜µä¹˜æ³•

### é—®é¢˜å®šä¹‰

P[i][j] = Î£(k=0â†’n-1) M[i][k] Ã— N[k][j]

- M: mÃ—n
- N: nÃ—o  
- P: mÃ—o

### åŸºç¡€å®ç°

æ¯ä¸ªçº¿ç¨‹è®¡ç®—ä¸€ä¸ªè¾“å‡ºå…ƒç´ ï¼š

```cuda
__global__ void matMul(float *M, float *N, float *P, 
                       int m, int n, int o) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < m && col < o) {
        float sum = 0.0f;
        for (int k = 0; k < n; k++) {
            sum += M[row * n + k] * N[k * o + col];
        }
        P[row * o + col] = sum;
    }
}
```

**ç´¢å¼•å…¬å¼**ï¼ˆè¡Œä¸»åºï¼‰ï¼š

- M[i][j] â†’ `i * width + j`
- ç¬¬iè¡Œç¬¬jåˆ—åœ¨ä¸€ç»´æ•°ç»„ä¸­çš„ä½ç½®

**å¯åŠ¨é…ç½®**ï¼š

```cuda
dim3 block(16, 16);
dim3 grid((o + 15) / 16, (m + 15) / 16);
matMul<<<grid, block>>>(d_M, d_N, d_P, m, n, o);
```

**è¾¹ç•Œæ£€æŸ¥**ï¼š`if (row < m && col < o)` ä¸¤ä¸ªæ¡ä»¶éƒ½è¦ï¼ˆgridå‘ä¸Šå–æ•´ä¼šå¤šçº¿ç¨‹ï¼‰

### æ€§èƒ½ç“¶é¢ˆ

**ç®—æœ¯å¼ºåº¦æä½**ï¼š

```
æ¯çº¿ç¨‹è¯»å–: 2nä¸ªfloat = 8n bytes
æ¯çº¿ç¨‹è®¡ç®—: 2n FLOP
ç®—æœ¯å¼ºåº¦ = 2n / 8n = 0.25 FLOP/Byte
```

å¯¹äºå³°å€¼10 TFLOPSã€å¸¦å®½500 GB/sçš„GPUï¼š

```
è¾¾åˆ°å³°å€¼éœ€è¦: 10T / 500G = 20 FLOP/Byte
å®é™…åªæœ‰: 0.25
åˆ©ç”¨ç‡: 1.25%
```

GPUå¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰æ•°æ®ã€‚æ›´ä¸¥é‡çš„æ˜¯æ•°æ®é‡å¤è¯»å–ï¼šM[i][k]è¢«ç¬¬iè¡Œçš„æ‰€æœ‰çº¿ç¨‹è¯»ï¼Œæ€»å…±è¢«è¯»mÃ—oæ¬¡ï¼Œä½†æ¯æ¬¡éƒ½ä»å…¨å±€å†…å­˜è¯»ï¼Œæ²¡æœ‰å¤ç”¨ã€‚

**ä¼˜åŒ–æ–¹å‘**ï¼ˆç¬¬4-5ç« ï¼‰ï¼šç”¨Shared MemoryåšTilingï¼Œç®—æœ¯å¼ºåº¦æå‡åˆ°10+ FLOP/Byteã€‚

### å˜ä½“å¯¹æ¯”

**æ¯çº¿ç¨‹ä¸€è¡Œ**ï¼š

```cuda
int row = blockIdx.x * blockDim.x + threadIdx.x;
for (int col = 0; col < size; col++) {
    // è®¡ç®—P[row][col]
}
```

**æ¯çº¿ç¨‹ä¸€åˆ—**ï¼šç±»ä¼¼ï¼Œå¤–å±‚å¾ªç¯æ”¹ä¸ºrow

| æ–¹æ¡ˆ | å¹¶è¡Œåº¦ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|
| æ¯çº¿ç¨‹ä¸€å…ƒç´  | mÃ—o (æœ€é«˜) | é€šç”¨ |
| æ¯çº¿ç¨‹ä¸€è¡Œ | m | m >> o |
| æ¯çº¿ç¨‹ä¸€åˆ— | o | o >> m |

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ¯çº¿ç¨‹ä¸€å…ƒç´ çš„å¹¶è¡Œåº¦æœ€é«˜ã€‚

## çŸ©é˜µ-å‘é‡ä¹˜æ³•

A[i] = Î£ B[i][j] Ã— C[j]

```cuda
__global__ void matVecMul(float *B, float *C, float *A, 
                          int m, int n) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < m) {
        float sum = 0.0f;
        for (int j = 0; j < n; j++) {
            sum += B[row * n + j] * C[j];
        }
        A[row] = sum;
    }
}
```

åªéœ€ä¸€ç»´ï¼Œå› ä¸ºè¾“å‡ºæ˜¯å‘é‡ã€‚ç“¶é¢ˆæ˜¯å‘é‡Cè¢«æ‰€æœ‰çº¿ç¨‹é‡å¤è¯»å–ï¼Œå¯ç”¨Constant Memoryæˆ–Shared Memoryä¼˜åŒ–ã€‚

## å›¾åƒå¤„ç†

### RGBè½¬ç°åº¦

**å…¬å¼**ï¼šGray = 0.299R + 0.587G + 0.114B

**å†…å­˜å¸ƒå±€**ï¼ˆRGBäº¤é”™ï¼‰ï¼š

```
[R0 G0 B0][R1 G1 B1]...[R(w-1) G(w-1) B(w-1)]
[Rw Gw Bw]...
```

**Kernel**ï¼š

```cuda
__global__ void rgb2gray(unsigned char *rgb, unsigned char *gray,
                         int width, int height) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (row < height && col < width) {
        int rgbIdx = (row * width + col) * 3;
        int grayIdx = row * width + col;
        
        unsigned char r = rgb[rgbIdx];
        unsigned char g = rgb[rgbIdx + 1];
        unsigned char b = rgb[rgbIdx + 2];
        
        gray[grayIdx] = 0.299f*r + 0.587f*g + 0.114f*b;
    }
}
```

**æ€§èƒ½**ï¼šå¸¦å®½å—é™ï¼ˆè¯»3å­—èŠ‚ï¼Œå†™1å­—èŠ‚ï¼Œè®¡ç®—å¾ˆå°‘ï¼‰ï¼Œä½†è®¿é—®è¿ç»­ï¼Œcoalescingæ•ˆæœå¥½ã€‚

### é«˜æ–¯æ¨¡ç³Š

**æ¨¡æ¿è®¡ç®—**ï¼ˆStencilï¼‰ï¼šæ¯ä¸ªåƒç´ ç”±é‚»åŸŸåŠ æƒæ±‚å’Œ

3Ã—3é«˜æ–¯æ ¸ï¼š

```
1/16 * [1 2 1]
       [2 4 2]
       [1 2 1]
```

**Kernel**ï¼š

```cuda
__global__ void gaussianBlur(unsigned char *in, unsigned char *out,
                              int width, int height) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    
    // è¾¹ç•Œç›´æ¥å¤åˆ¶
    if (row == 0 || row == height-1 || 
        col == 0 || col == width-1) {
        if (row < height && col < width)
            out[row*width + col] = in[row*width + col];
        return;
    }
    
    if (row < height && col < width) {
        float sum = 0;
        // 3x3é‚»åŸŸ
        sum += in[(row-1)*width + col-1] * 1.0f;
        sum += in[(row-1)*width + col  ] * 2.0f;
        sum += in[(row-1)*width + col+1] * 1.0f;
        sum += in[row*width + col-1] * 2.0f;
        sum += in[row*width + col  ] * 4.0f;
        sum += in[row*width + col+1] * 2.0f;
        sum += in[(row+1)*width + col-1] * 1.0f;
        sum += in[(row+1)*width + col  ] * 2.0f;
        sum += in[(row+1)*width + col+1] * 1.0f;
        
        out[row*width + col] = sum / 16.0f;
    }
}
```

**å…³é”®ç‚¹**ï¼š
-è¾¹ç•Œå¤„ç†ï¼šè¿™é‡Œç®€åŒ–ä¸ºå¤åˆ¶ï¼Œä¹Ÿå¯padding/é•œåƒ

- æ•°æ®é‡ç”¨ï¼šç›¸é‚»åƒç´ çš„é‚»åŸŸé‡å ï¼Œä½†æ­¤ç‰ˆæœ¬æ²¡å¤ç”¨
- ä¼˜åŒ–ï¼šç”¨Shared Memoryè®©blockåä½œåŠ è½½tile+halo

## å†…å­˜å¸ƒå±€

### è¡Œä¸»åºï¼ˆC/CUDAï¼‰

å…ƒç´ [row][col]çš„ç´¢å¼•ï¼š

```cuda
index = row * width + col
```

ä¾‹ï¼š4Ã—3çŸ©é˜µå…ƒç´ [2][1]ï¼š`2*3 + 1 = 7`

### åˆ—ä¸»åºï¼ˆFortran/MATLABï¼‰

```cuda
index = col * height + row
```

åŒæ ·å…ƒç´ [2][1]ï¼š`1*4 + 2 = 6`

### ä¸‰ç»´å¼ é‡

æ·±åº¦Ã—é«˜åº¦Ã—å®½åº¦ï¼Œå…ƒç´ [z][y][x]ï¼š

```cuda
index = z * (height * width) + y * width + x
```

ä¾‹ï¼š300Ã—500Ã—400 (DÃ—HÃ—W)ï¼Œå…ƒç´ [5][20][10]ï¼š

```
5*(500*400) + 20*400 + 10 = 1,008,010
```

**æ³¨æ„**ï¼šæ·±åº¦å­¦ä¹ æ¡†æ¶ç»´åº¦é¡ºåºå¯èƒ½ä¸åŒï¼ˆPyTorchç”¨NCHWï¼ŒTensorFlowç”¨NHWCï¼‰ã€‚

## æ‰§è¡Œé…ç½®ç¤ºä¾‹

ä¹¦ä¸­ä¹ é¢˜3ï¼š

```cuda
dim3 bd(16, 32);                      // block: 16Ã—32
dim3 gd((300-1)/16+1, (150-1)/32+1); // grid
// M=150, N=300
```

- æ¯blockï¼š16Ã—32 = 512 threads
- Gridï¼š(19, 5) = 95 blocks
- æ€»çº¿ç¨‹ï¼š95Ã—512 = 48,640
- æœ‰æ•ˆçº¿ç¨‹ï¼ˆrow<150 && col<300ï¼‰ï¼š150Ã—300 = 45,000
- æµªè´¹ï¼š3,640 (7.5%)ï¼Œå¯æ¥å—

## æ€§èƒ½åˆ†ææ€è·¯

### Rooflineæ¨¡å‹

```
å®é™…æ€§èƒ½ = min(è®¡ç®—å³°å€¼, å¸¦å®½ Ã— ç®—æœ¯å¼ºåº¦)
```

è¦è¾¾åˆ°è®¡ç®—å³°å€¼éœ€è¦ï¼šç®—æœ¯å¼ºåº¦ â‰¥ å³°å€¼FLOPS / å¸¦å®½

ä¾‹å¦‚GPU 10 TFLOPS, 500 GB/sï¼šéœ€è¦ â‰¥ 20 FLOP/Byte

å½“å‰ç®—æ³•éƒ½è¿œä½äºæ­¤ï¼Œæ€§èƒ½å—å†…å­˜é™åˆ¶ã€‚

### æå‡ç®—æœ¯å¼ºåº¦

æ ¸å¿ƒï¼š**æ•°æ®é‡ç”¨**

- **Tiling**ï¼šæ•°æ®åŠ è½½åˆ°Shared Memoryé‡å¤ä½¿ç”¨
- **å¯„å­˜å™¨å¤ç”¨**ï¼šæ¯çº¿ç¨‹è®¡ç®—å¤šä¸ªå…ƒç´ 
- **Kernelèåˆ**ï¼šå‡å°‘ä¸­é—´ç»“æœä¼ è¾“

çŸ©é˜µä¹˜æ³•ä¼˜åŒ–ç‰ˆèƒ½è¾¾åˆ°10+ FLOP/Byteï¼Œæ€§èƒ½æå‡10å€ä»¥ä¸Šã€‚

## å°ç»“

ç¬¬ä¸‰ç« ä»ä¸€ç»´æ‰©å±•åˆ°å¤šç»´ï¼Œæ ¸å¿ƒæ˜¯Grid/Blockçš„çµæ´»ç»„ç»‡ï¼š

**å¤šç»´ç´¢å¼•**ï¼š`row = blockIdx.y*blockDim.y + threadIdx.y` è¦ç†Ÿç»ƒï¼ŒäºŒç»´ã€ä¸‰ç»´æ˜¯è‡ªç„¶æ‰©å±•ã€‚

**å†…å­˜å¸ƒå±€**ï¼šè¡Œä¸»åºå†³å®šç´¢å¼•å…¬å¼ `row*width + col`ï¼Œæé”™ä¼šè®¿é—®é”™è¯¯æ•°æ®ã€‚

**æ€§èƒ½è®¤çŸ¥**ï¼šåŸºç¡€å®ç°éƒ½æ˜¯å†…å­˜å—é™ï¼Œç®—æœ¯å¼ºåº¦0.1-0.25 FLOP/Byteï¼Œè¿œä½äºéœ€è¦çš„20ã€‚GPUè®¡ç®—èƒ½åŠ›å¼ºï¼Œä½†æ•°æ®ä¾›åº”è·Ÿä¸ä¸Šã€‚

**ä¼˜åŒ–æ–¹å‘**ï¼šæå‡ç®—æœ¯å¼ºåº¦ = å¢åŠ æ•°æ®é‡ç”¨ã€‚è¿™æ˜¯ç¬¬4-5ç« çš„é‡ç‚¹ã€‚

**ä»£ç ä¹ æƒ¯**ï¼š

- äºŒç»´/ä¸‰ç»´éƒ½è¦ä¸¥æ ¼è¾¹ç•Œæ£€æŸ¥
- ç´¢å¼•è®¡ç®—å…ˆrowåcolï¼ˆè¡Œä¸»åºï¼‰
- æ€§èƒ½åˆ†æè¦é‡åŒ–ï¼ˆç®—æœ¯å¼ºåº¦ã€å¸¦å®½åˆ©ç”¨ç‡ï¼‰

ç†è§£äº†æœ´ç´ å®ç°çš„ç“¶é¢ˆï¼Œæ‰èƒ½æ˜ç™½å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰å’Œåˆ†å—ï¼ˆTilingï¼‰çš„ä»·å€¼ã€‚ç¬¬äº”ç« çš„ä¼˜åŒ–æŠ€æœ¯ä¼šè®©åŒæ ·çš„çŸ©é˜µä¹˜æ³•æ€§èƒ½æå‡10å€ä»¥ä¸Šã€‚

---

**å‚è€ƒèµ„æ–™ï¼š**

- Hwu, W., Kirk, D., & El Hajj, I. (2022). *Programming Massively Parallel Processors: A Hands-on Approach* (4th Edition). Morgan Kaufmann.
- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

---

> **æœ¬æ–‡ GitHub ä»“åº“**: [https://github.com/psmarter/PMPP-Learning](https://github.com/psmarter/PMPP-Learning)
