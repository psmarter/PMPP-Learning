---
title: PMPP-ç¬¬åå…­ç« ï¼šæ·±åº¦å­¦ä¹ 
date: 2026-01-19 23:07:59
tags:
  - CUDA
  - GPUç¼–ç¨‹
  - å¹¶è¡Œè®¡ç®—
  - PMPP
  - æ·±åº¦å­¦ä¹ 
  - å·ç§¯ç¥ç»ç½‘ç»œ
  - cuDNN
categories: çŸ¥è¯†åˆ†äº«
cover: /img/PMPP.jpg
---

## å‰è¨€

å‰é¢çš„ç« èŠ‚å­¦ä¹ äº†å„ç§å¹¶è¡Œç®—æ³•åŸè¯­ï¼šå½’çº¦ã€æ‰«æã€æ’åºã€ç¨€ç–çŸ©é˜µã€å›¾éå†ã€‚è¿™äº›æŠ€æœ¯åœ¨æ·±åº¦å­¦ä¹ ä¸­éƒ½æœ‰åº”ç”¨ã€‚ç¬¬åå…­ç« å°†è¿™äº›æŠ€æœ¯ä¸²è”èµ·æ¥ï¼Œå±•ç¤º GPU å¦‚ä½•åŠ é€Ÿ**æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰**â€”â€”å½“ä»Š GPU æœ€é‡è¦çš„åº”ç”¨é¢†åŸŸä¹‹ä¸€ã€‚æœ¬ç« ä¸æ·±å…¥è®²è§£ç¥ç»ç½‘ç»œç†è®ºï¼Œè€Œæ˜¯èšç„¦äº**è®¡ç®—è§†è§’**ï¼šç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ“ä½œæ˜¯ä»€ä¹ˆï¼ŸGPU å¦‚ä½•é«˜æ•ˆå®ç°è¿™äº›æ“ä½œï¼Ÿ

> **ğŸ“¦ é…å¥—èµ„æº**ï¼šæœ¬ç³»åˆ—æ–‡ç« é…æœ‰å®Œæ•´çš„ [GitHub ä»“åº“](https://github.com/psmarter/PMPP-Learning)ï¼ŒåŒ…å«æ¯ç« çš„ç»ƒä¹ é¢˜è§£ç­”ã€CUDA ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚æ‰€æœ‰ä»£ç éƒ½ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œã€‚

## æ·±åº¦å­¦ä¹ åŸºç¡€

### ç¥ç»ç½‘ç»œçš„è®¡ç®—æœ¬è´¨

æ— è®ºæ˜¯å…¨è¿æ¥å±‚ã€å·ç§¯å±‚è¿˜æ˜¯æ³¨æ„åŠ›å±‚ï¼Œç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ“ä½œéƒ½æ˜¯ï¼š

1. **çº¿æ€§å˜æ¢**ï¼šçŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰
2. **éçº¿æ€§æ¿€æ´»**ï¼šReLUã€Sigmoidã€Softmax ç­‰
3. **å½’çº¦æ“ä½œ**ï¼šPoolingã€Normalization

**è®¡ç®—é‡åˆ†å¸ƒ**ï¼ˆä»¥ ResNet-50 ä¸ºä¾‹ï¼‰ï¼š

- å·ç§¯å±‚ï¼š~95% çš„è®¡ç®—é‡
- å…¨è¿æ¥å±‚ï¼š~4%
- å…¶ä»–ï¼š~1%

å·ç§¯æ‰æ˜¯ GPU ä¼˜åŒ–çš„ä¸»æˆ˜åœºã€‚

### æ·±åº¦å­¦ä¹ ä¸ºä»€ä¹ˆéœ€è¦ GPU

| ç‰¹æ€§           | CPU       | GPU        |
| -------------- | --------- | ---------- |
| æ ¸å¿ƒæ•°         | 8-64      | æ•°åƒ       |
| æ—¶é’Ÿé¢‘ç‡       | 3-5 GHz   | 1-2 GHz    |
| å³°å€¼ç®—åŠ›(FP32) | ~1 TFLOPS | ~30 TFLOPS |
| å†…å­˜å¸¦å®½       | ~100 GB/s | ~1000 GB/s |

æ·±åº¦å­¦ä¹ çš„è®¡ç®—æ˜¯**é«˜åº¦å¹¶è¡Œ**çš„ï¼š

- æ‰¹é‡å¤„ç†ï¼ˆBatchï¼‰ï¼šç‹¬ç«‹æ ·æœ¬å¹¶è¡Œ
- ç©ºé—´å¹¶è¡Œï¼šå›¾åƒä¸åŒä½ç½®å¹¶è¡Œ
- é€šé“å¹¶è¡Œï¼šä¸åŒç‰¹å¾å›¾å¹¶è¡Œ

è¿™æ˜¯ GPU çš„ç†æƒ³åœºæ™¯ã€‚

## å·ç§¯å±‚çš„ GPU å®ç°

### å·ç§¯çš„è®¡ç®—å¤æ‚åº¦

ç»™å®šè¾“å…¥å¼ é‡ `[N, C_in, H, W]` å’Œå·ç§¯æ ¸ `[C_out, C_in, K, K]`ï¼š

$$
\text{FLOPs} = 2 \times N \times C_{out} \times H_{out} \times W_{out} \times C_{in} \times K \times K
$$

å¯¹äº ResNet-50 çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼š

- è¾“å…¥ï¼š`[1, 3, 224, 224]`
- å·ç§¯æ ¸ï¼š`[64, 3, 7, 7]`
- FLOPs â‰ˆ 1.2 äº¿æ¬¡

æ•´ä¸ªç½‘ç»œçº¦éœ€ 40 äº¿æ¬¡æµ®ç‚¹æ“ä½œã€‚

### ç›´æ¥å·ç§¯

æœ€ç›´è§‚çš„å®ç°ï¼Œç¬¬ä¸ƒç« å·²è¯¦ç»†è®²è¿‡ï¼š

```cuda
__global__ void conv2d_direct(float *input, float *kernel, float *output,
                               int N, int C_in, int H, int W, 
                               int C_out, int K) {
    int n = blockIdx.z;
    int c_out = blockIdx.y;
    int h_out = blockIdx.x * blockDim.x + threadIdx.x;
    int w_out = blockIdx.x * blockDim.y + threadIdx.y;
    
    if (h_out < H_out && w_out < W_out) {
        float sum = 0.0f;
        for (int c_in = 0; c_in < C_in; c_in++) {
            for (int kh = 0; kh < K; kh++) {
                for (int kw = 0; kw < K; kw++) {
                    int h_in = h_out + kh - K/2;
                    int w_in = w_out + kw - K/2;
                    if (h_in >= 0 && h_in < H && w_in >= 0 && w_in < W) {
                        sum += input[...] * kernel[...];
                    }
                }
            }
        }
        output[...] = sum;
    }
}
```

**é—®é¢˜**ï¼šå¾ªç¯å¤ªå¤šï¼Œå†…å­˜è®¿é—®ä¸è§„åˆ™ï¼Œæ•ˆç‡ä½ã€‚

### Im2col + GEMM

**æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠå·ç§¯è½¬åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ã€‚

**Im2col**ï¼šæŠŠè¾“å…¥çš„æ¯ä¸ªæ»‘åŠ¨çª—å£å±•å¼€æˆä¸€åˆ—ã€‚

```
è¾“å…¥å›¾åƒ [C_in, H, W]ï¼Œå·ç§¯æ ¸ [K, K]

Im2col åï¼š
  åˆ—æ•° = H_out Ã— W_out
  è¡Œæ•° = C_in Ã— K Ã— K

æ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ªæ»‘åŠ¨çª—å£çš„å±•å¼€
```

**è½¬æ¢å**ï¼š

```
è¾“å‡º = Kernel Ã— Im2col(Input)
[C_out, H_outÃ—W_out] = [C_out, C_inÃ—KÃ—K] Ã— [C_inÃ—KÃ—K, H_outÃ—W_out]
```

è¿™æ˜¯æ ‡å‡†çš„ GEMMï¼å¯ä»¥è°ƒç”¨é«˜åº¦ä¼˜åŒ–çš„ cuBLASã€‚

**ä»£ä»·**ï¼šIm2col éœ€è¦é¢å¤–å†…å­˜ï¼Œçº¦ä¸ºåŸè¾“å…¥çš„ KÂ² å€ã€‚

### Winograd å·ç§¯

**æ ¸å¿ƒæ€æƒ³**ï¼šç”¨æ›´å¤šåŠ æ³•æ¢æ›´å°‘ä¹˜æ³•ã€‚

å¯¹äº 3Ã—3 å·ç§¯ï¼ŒWinograd F(2Ã—2, 3Ã—3) å¯ä»¥æŠŠä¹˜æ³•æ¬¡æ•°ä» 36 å‡å°‘åˆ° 16ã€‚

**å…¬å¼**ï¼š

$$
Y = A^T \left[ (G \cdot g \cdot G^T) \odot (B^T \cdot d \cdot B) \right] A
$$

å…¶ä¸­ï¼š

- $g$ï¼šå·ç§¯æ ¸
- $d$ï¼šè¾“å…¥ Tile
- $G, B, A$ï¼šå˜æ¢çŸ©é˜µ
- $\odot$ï¼šé€å…ƒç´ ä¹˜æ³•

**ä¼˜åŠ¿**ï¼šè®¡ç®—é‡å‡å°‘ 2.25 å€ï¼ˆç†è®ºä¸Šï¼‰ã€‚

**é™åˆ¶**ï¼š

- åªé€‚åˆå°å·ç§¯æ ¸ï¼ˆ3Ã—3 æœ€å¸¸ç”¨ï¼‰
- æ•°å€¼ç¨³å®šæ€§é—®é¢˜
- å®ç°å¤æ‚

### FFT å·ç§¯

**æ ¸å¿ƒæ€æƒ³**ï¼šå·ç§¯å®šç†â€”â€”æ—¶åŸŸå·ç§¯ = é¢‘åŸŸé€å…ƒç´ ä¹˜æ³•ã€‚

```
Output = IFFT(FFT(Input) âŠ™ FFT(Kernel))
```

**å¤æ‚åº¦**ï¼šO(NÂ² log N) vs ç›´æ¥å·ç§¯çš„ O(NÂ² KÂ²)

**é€‚ç”¨åœºæ™¯**ï¼šå¤§å·ç§¯æ ¸ï¼ˆK > 7ï¼‰ã€‚æ·±åº¦å­¦ä¹ ä¸­å·ç§¯æ ¸é€šå¸¸å¾ˆå°ï¼ˆ1Ã—1, 3Ã—3ï¼‰ï¼Œæ‰€ä»¥ FFT å·ç§¯è¾ƒå°‘ä½¿ç”¨ã€‚

### cuDNN ç­–ç•¥é€‰æ‹©

cuDNN å†…ç½®å¤šç§å·ç§¯ç®—æ³•ï¼Œä¼šæ ¹æ®é—®é¢˜è§„æ¨¡è‡ªåŠ¨é€‰æ‹©ï¼š

| ç®—æ³•                  | é€‚ç”¨åœºæ™¯ |
| --------------------- | -------- |
| IMPLICIT_GEMM         | é€šç”¨     |
| IMPLICIT_PRECOMP_GEMM | å¤§ Batch |
| GEMM                  | å°å·ç§¯æ ¸ |
| WINOGRAD              | 3Ã—3 å·ç§¯ |
| FFT                   | å¤§å·ç§¯æ ¸ |

```cuda
cudnnConvolutionFwdAlgoPerf_t perfResults[8];
cudnnFindConvolutionForwardAlgorithm(
    handle, inputDesc, filterDesc, convDesc, outputDesc,
    8, &returnedAlgoCount, perfResults);

// é€‰æ‹©æœ€å¿«çš„ç®—æ³•
cudnnConvolutionFwdAlgo_t algo = perfResults[0].algo;
```

## å…¨è¿æ¥å±‚

### æœ¬è´¨ï¼šçŸ©é˜µä¹˜æ³•

```
y = W Ã— x + b
[out_features] = [out_features, in_features] Ã— [in_features] + [out_features]
```

æ‰¹é‡å¤„ç†ï¼š

```
Y = X Ã— W^T + B
[batch, out] = [batch, in] Ã— [in, out] + [batch, out]
```

ç›´æ¥è°ƒç”¨ cuBLAS GEMM å³å¯ã€‚

### ä¼˜åŒ–ï¼šTensor Core

ä» Volta æ¶æ„å¼€å§‹ï¼ŒGPU æœ‰ä¸“é—¨çš„ Tensor Coreï¼š

| æ“ä½œ           | CUDA Core | Tensor Core |
| -------------- | --------- | ----------- |
| 4Ã—4 çŸ©é˜µä¹˜åŠ    | 128 FLOPs | 1 å‘¨æœŸ      |
| ç²¾åº¦           | FP32      | FP16/BF16   |
| å³°å€¼ç®—åŠ›(A100) | 19 TFLOPS | 312 TFLOPS  |

ä½¿ç”¨ Tensor Core éœ€è¦ï¼š

1. æ•°æ®ç±»å‹ä¸º FP16 æˆ– BF16
2. çŸ©é˜µç»´åº¦æ˜¯ 8 æˆ– 16 çš„å€æ•°
3. ä½¿ç”¨ cuBLAS æˆ– WMMA API

## æ¿€æ´»å‡½æ•°

### ReLU

```cuda
__global__ void relu(float *x, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        x[i] = max(x[i], 0.0f);
    }
}
```

è®¡ç®—ç®€å•ï¼Œå±äº **Memory-Bound**ï¼šè®¿å­˜æ—¶é—´è¿œå¤§äºè®¡ç®—æ—¶é—´ã€‚

**Fusion ä¼˜åŒ–**ï¼šæŠŠ ReLU èåˆåˆ°å·ç§¯ Kernel ä¸­ï¼Œé¿å…é¢å¤–çš„å†…å­˜è¯»å†™ã€‚

```cuda
// å·ç§¯ç»“æœç›´æ¥åº”ç”¨ ReLU
output[idx] = max(conv_result, 0.0f);
```

### Softmax

```cuda
// æ•°å€¼ç¨³å®šç‰ˆæœ¬
__global__ void softmax(float *x, float *y, int n) {
    // 1. æ‰¾æœ€å¤§å€¼ï¼ˆè§„çº¦ï¼‰
    float max_val = reduce_max(x, n);
    
    // 2. exp(x - max) å¹¶æ±‚å’Œï¼ˆè§„çº¦ï¼‰
    float sum = 0;
    for (int i = threadIdx.x; i < n; i += blockDim.x) {
        sum += expf(x[i] - max_val);
    }
    sum = reduce_sum(sum);
    
    // 3. å½’ä¸€åŒ–
    for (int i = threadIdx.x; i < n; i += blockDim.x) {
        y[i] = expf(x[i] - max_val) / sum;
    }
}
```

éœ€è¦ä¸¤æ¬¡å½’çº¦ï¼Œä¸‰æ¬¡éå†æ•°æ®ã€‚

## Pooling å±‚

### Max Pooling

```cuda
__global__ void max_pool2d(float *input, float *output,
                            int H, int W, int pool_size, int stride) {
    int h_out = blockIdx.y * blockDim.y + threadIdx.y;
    int w_out = blockIdx.x * blockDim.x + threadIdx.x;
    
    float max_val = -FLT_MAX;
    for (int ph = 0; ph < pool_size; ph++) {
        for (int pw = 0; pw < pool_size; pw++) {
            int h_in = h_out * stride + ph;
            int w_in = w_out * stride + pw;
            max_val = max(max_val, input[h_in * W + w_in]);
        }
    }
    output[h_out * W_out + w_out] = max_val;
}
```

ç±»ä¼¼äºå·ç§¯ï¼Œä½†ç”¨ max æ›¿ä»£åŠ æƒå’Œã€‚

### Global Average Pooling

å¯¹æ•´ä¸ªç‰¹å¾å›¾æ±‚å¹³å‡ï¼š

```cuda
// å°±æ˜¯ä¸€ä¸ª 2D å½’çº¦ï¼
float sum = reduce_2d(feature_map, H, W);
output = sum / (H * W);
```

ç¬¬åç« çš„å½’çº¦æŠ€æœ¯ç›´æ¥é€‚ç”¨ã€‚

## Batch Normalization

### è®¡ç®—è¿‡ç¨‹

1. **è®¡ç®—å‡å€¼**ï¼š$\mu = \frac{1}{m} \sum x_i$ï¼ˆå½’çº¦ï¼‰
2. **è®¡ç®—æ–¹å·®**ï¼š$\sigma^2 = \frac{1}{m} \sum (x_i - \mu)^2$ï¼ˆå½’çº¦ï¼‰
3. **å½’ä¸€åŒ–**ï¼š$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}$ï¼ˆé€å…ƒç´ ï¼‰
4. **ç¼©æ”¾å¹³ç§»**ï¼š$y = \gamma \hat{x} + \beta$ï¼ˆé€å…ƒç´ ï¼‰

### GPU å®ç°è¦ç‚¹

```cuda
__global__ void batch_norm(float *x, float *y, 
                           float *gamma, float *beta,
                           int N, int C, int H, int W) {
    int c = blockIdx.x;  // æ¯ä¸ª Block å¤„ç†ä¸€ä¸ªé€šé“
    
    // 1. è®¡ç®—è¯¥é€šé“çš„å‡å€¼å’Œæ–¹å·®
    float mean = compute_mean(x, c, N, H, W);
    float var = compute_var(x, c, mean, N, H, W);
    
    __syncthreads();
    
    // 2. å½’ä¸€åŒ–å¹¶ç¼©æ”¾
    for (int i = threadIdx.x; i < N * H * W; i += blockDim.x) {
        float val = x[index(i, c)];
        y[index(i, c)] = gamma[c] * (val - mean) / sqrtf(var + 1e-5f) + beta[c];
    }
}
```

å…³é”®æ˜¯**æŒ‰é€šé“å½’çº¦**ï¼Œæ¯ä¸ªé€šé“ç‹¬ç«‹å¤„ç†ã€‚

## åå‘ä¼ æ’­

### è®¡ç®—å›¾ä¸è‡ªåŠ¨å¾®åˆ†

æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆPyTorchã€TensorFlowï¼‰é€šè¿‡**è®¡ç®—å›¾**è¿½è¸ªæ“ä½œï¼Œè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

**å‰å‘ä¼ æ’­**ï¼š

```
x â†’ Conv â†’ ReLU â†’ Pool â†’ FC â†’ Softmax â†’ Loss
```

**åå‘ä¼ æ’­**ï¼š

```
dLoss/dL â† dL/dFC â† dFC/dPool â† dPool/dReLU â† dReLU/dConv â† dConv/dx
```

æ¯ä¸ªæ“ä½œéƒ½æœ‰å¯¹åº”çš„æ¢¯åº¦è®¡ç®— Kernelã€‚

### å·ç§¯çš„åå‘ä¼ æ’­

å·ç§¯çš„æ¢¯åº¦è®¡ç®—ä¹Ÿæ˜¯å·ç§¯ï¼

```
# å¯¹è¾“å…¥çš„æ¢¯åº¦
dL/dInput = Convolution(dL/dOutput, Kernel^T)

# å¯¹æƒé‡çš„æ¢¯åº¦
dL/dKernel = Convolution(Input, dL/dOutput)
```

æ‰€ä»¥å·ç§¯çš„ä¼˜åŒ–åŒæ ·é€‚ç”¨äºåå‘ä¼ æ’­ã€‚

## cuDNN æ¥å£

### åŸºæœ¬ä½¿ç”¨

```cuda
#include <cudnn.h>

// 1. åˆ›å»ºå¥æŸ„
cudnnHandle_t handle;
cudnnCreate(&handle);

// 2. åˆ›å»ºå¼ é‡æè¿°ç¬¦
cudnnTensorDescriptor_t inputDesc, outputDesc;
cudnnCreateTensorDescriptor(&inputDesc);
cudnnSetTensor4dDescriptor(inputDesc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT,
                           N, C, H, W);

// 3. åˆ›å»ºå·ç§¯æè¿°ç¬¦
cudnnConvolutionDescriptor_t convDesc;
cudnnCreateConvolutionDescriptor(&convDesc);
cudnnSetConvolution2dDescriptor(convDesc, pad, pad, stride, stride, 1, 1,
                                 CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT);

// 4. æŸ¥è¯¢å·¥ä½œç©ºé—´å¤§å°
size_t workspaceSize;
cudnnGetConvolutionForwardWorkspaceSize(handle, inputDesc, filterDesc,
                                         convDesc, outputDesc, algo, &workspaceSize);

// 5. åˆ†é…å·¥ä½œç©ºé—´å¹¶æ‰§è¡Œ
void *workspace;
cudaMalloc(&workspace, workspaceSize);

float alpha = 1.0f, beta = 0.0f;
cudnnConvolutionForward(handle, &alpha, inputDesc, input,
                        filterDesc, filter, convDesc, algo,
                        workspace, workspaceSize,
                        &beta, outputDesc, output);
```

### å¸¸ç”¨æ“ä½œ

| æ“ä½œ             | å‡½æ•°                            |
| ---------------- | ------------------------------- |
| å·ç§¯å‰å‘         | cudnnConvolutionForward         |
| å·ç§¯åå‘ï¼ˆæ•°æ®ï¼‰ | cudnnConvolutionBackwardData    |
| å·ç§¯åå‘ï¼ˆæƒé‡ï¼‰ | cudnnConvolutionBackwardFilter  |
| æ± åŒ–             | cudnnPoolingForward/Backward    |
| æ¿€æ´»             | cudnnActivationForward/Backward |
| BatchNorm        | cudnnBatchNormForward/Backward  |
| Softmax          | cudnnSoftmaxForward/Backward    |

## æ··åˆç²¾åº¦è®­ç»ƒ

### FP16 çš„ä¼˜åŠ¿

| ç²¾åº¦ | å­˜å‚¨   | å¸¦å®½ | ç®—åŠ›(A100) |
| ---- | ------ | ---- | ---------- |
| FP32 | 4 å­—èŠ‚ | 1Ã—   | 19 TFLOPS  |
| FP16 | 2 å­—èŠ‚ | 2Ã—   | 312 TFLOPS |

### è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰

```python
# PyTorch ç¤ºä¾‹
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for data, target in dataloader:
    optimizer.zero_grad()
    
    with autocast():  # è‡ªåŠ¨è½¬æ¢åˆ° FP16
        output = model(data)
        loss = criterion(output, target)
    
    scaler.scale(loss).backward()  # æ¢¯åº¦ç¼©æ”¾
    scaler.step(optimizer)
    scaler.update()
```

**Loss Scaling**ï¼šé˜²æ­¢ FP16 æ¢¯åº¦ä¸‹æº¢ï¼Œå…ˆæ”¾å¤§ Lossï¼Œè®¡ç®—å®Œæ¢¯åº¦åç¼©å°ã€‚

## å°ç»“

ç¬¬åå…­ç« å±•ç¤ºäº†æ·±åº¦å­¦ä¹ ä¸ GPU å¹¶è¡Œè®¡ç®—çš„ç´§å¯†è”ç³»ï¼š

**æ ¸å¿ƒæ“ä½œ**ï¼šæ·±åº¦å­¦ä¹ çš„è®¡ç®—æœ¬è´¨æ˜¯çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰å’Œå·ç§¯ã€‚è¿™ä¸¤ä¸ªæ“ä½œå æ®äº† 95% ä»¥ä¸Šçš„è®¡ç®—é‡ã€‚

**å·ç§¯å®ç°**ï¼šç›´æ¥å·ç§¯ç®€å•ä½†ä½æ•ˆï¼›Im2col + GEMM è½¬åŒ–ä¸ºçŸ©é˜µä¹˜æ³•ï¼›Winograd å‡å°‘ä¹˜æ³•æ¬¡æ•°ï¼›cuDNN è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥ã€‚

**èåˆä¼˜åŒ–**ï¼šæŠŠå¤šä¸ªæ“ä½œèåˆåˆ°ä¸€ä¸ª Kernel ä¸­ï¼Œå‡å°‘å†…å­˜è®¿é—®ã€‚ReLUã€BatchNorm ç­‰å¸¸ä¸å·ç§¯èåˆã€‚

**Tensor Core**ï¼šä¸“ç”¨ç¡¬ä»¶åŠ é€ŸçŸ©é˜µè¿ç®—ï¼ŒFP16 å³°å€¼ç®—åŠ›æ˜¯ FP32 çš„ 16 å€ã€‚æ··åˆç²¾åº¦è®­ç»ƒå·²æˆä¸ºæ ‡å‡†åšæ³•ã€‚

**cuDNN**ï¼šå°è£…äº†æ‰€æœ‰ä¼˜åŒ–ï¼Œæ˜¯æ·±åº¦å­¦ä¹ æ¡†æ¶çš„åº•å±‚ä¾èµ–ã€‚ç†è§£å…¶åŸç†æœ‰åŠ©äºè°ƒä¼˜å’Œè°ƒè¯•ã€‚

æ·±åº¦å­¦ä¹ æ˜¯ GPU è®¡ç®—çš„"æ€æ‰‹çº§åº”ç”¨"ï¼Œæ­£æ˜¯è¿™ä¸€éœ€æ±‚æ¨åŠ¨äº† GPU ç¡¬ä»¶å’Œè½¯ä»¶çš„é£é€Ÿå‘å±•ã€‚æŒæ¡æœ¬ç« å†…å®¹ï¼Œä½ å°±èƒ½ç†è§£ PyTorchã€TensorFlow åœ¨åº•å±‚æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

## ğŸš€ ä¸‹ä¸€æ­¥

- æ·±å…¥å­¦ä¹  cuDNN çš„ APIï¼Œå°è¯•æ‰‹åŠ¨è°ƒç”¨ä¸åŒçš„å·ç§¯ç®—æ³•å¹¶å¯¹æ¯”æ€§èƒ½
- å®ç°ä¸€ä¸ªç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œä»é›¶å¼€å§‹æ„å»ºå‰å‘å’Œåå‘ä¼ æ’­
- æ¢ç´¢æ··åˆç²¾åº¦è®­ç»ƒï¼Œäº†è§£ FP16/BF16 çš„ä½¿ç”¨åœºæ™¯å’Œæ³¨æ„äº‹é¡¹
- å­¦ä¹  Tensor Core ç¼–ç¨‹ï¼Œä½¿ç”¨ WMMA API å®ç°è‡ªå®šä¹‰çš„çŸ©é˜µä¹˜æ³•
- ç ”ç©¶ç®—å­èåˆæŠ€æœ¯ï¼Œå°†å¤šä¸ªæ“ä½œåˆå¹¶åˆ°ä¸€ä¸ª Kernel ä¸­

---

## ğŸ“š å‚è€ƒèµ„æ–™

- PMPP ç¬¬å››ç‰ˆ Chapter 16
- [ç¬¬åå…­ç« ï¼šæ·±åº¦å­¦ä¹ ](https://smarter.xin/posts/pmmpp-chapter16-deep-learning/)
- Hwu, W., Kirk, D., & El Hajj, I. (2022). *Programming Massively Parallel Processors: A Hands-on Approach* (4th Edition). Morgan Kaufmann.
- [NVIDIA cuDNN Developer Guide](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/)
- Lavin, A., & Gray, S. (2016). *Fast Algorithms for Convolutional Neural Networks*. CVPR.

**å­¦ä¹ æ„‰å¿«ï¼** ğŸ“

---

> **æœ¬æ–‡ GitHub ä»“åº“**: [https://github.com/psmarter/PMPP-Learning](https://github.com/psmarter/PMPP-Learning)
