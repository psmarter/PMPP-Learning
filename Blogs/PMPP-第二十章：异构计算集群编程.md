---
title: PMPP-ç¬¬äºŒåç« ï¼šå¼‚æ„è®¡ç®—é›†ç¾¤ç¼–ç¨‹
date: 2026-01-24 15:26:44
tags:
  - CUDA
  - GPUç¼–ç¨‹
  - å¹¶è¡Œè®¡ç®—
  - PMPP
  - MPI
  - é›†ç¾¤è®¡ç®—
  - CUDAæµ
  - åˆ†å¸ƒå¼è®¡ç®—
categories: çŸ¥è¯†åˆ†äº«
cover: /img/PMPP.jpg
---

## å‰è¨€

ç¬¬åä¹ç« æ€»ç»“äº†å¹¶è¡Œç¼–ç¨‹çš„æ€ç»´æ–¹æ³•ã€‚ç¬¬äºŒåç« å°†è§†é‡æ‰©å±•åˆ°**è®¡ç®—é›†ç¾¤ï¼ˆComputing Clusterï¼‰**â€”â€”å¤šå°è®¡ç®—æœºé€šè¿‡é«˜é€Ÿç½‘ç»œè¿æ¥ï¼Œæ¯å°è®¡ç®—æœºå¯èƒ½é…å¤‡å¤šä¸ª GPUã€‚è¿™æ˜¯å½“ä»Šè¶…çº§è®¡ç®—æœºå’Œæ•°æ®ä¸­å¿ƒçš„å…¸å‹æ¶æ„ã€‚æœ¬ç« è®¨è®ºå¦‚ä½•ä½¿ç”¨ **MPIï¼ˆMessage Passing Interfaceï¼Œæ¶ˆæ¯ä¼ é€’æ¥å£ï¼‰**ä¸ CUDA ç»“åˆï¼Œå®ç°è·¨èŠ‚ç‚¹çš„å¼‚æ„å¹¶è¡Œè®¡ç®—ã€‚æŒæ¡è¿™äº›æŠ€æœ¯ï¼Œå°±èƒ½ç¼–å†™å¯æ‰©å±•åˆ°æ•°åƒä¸ª GPU çš„å¤§è§„æ¨¡å¹¶è¡Œç¨‹åºã€‚

> **ğŸ“¦ é…å¥—èµ„æº**ï¼šæœ¬ç³»åˆ—æ–‡ç« é…æœ‰å®Œæ•´çš„ [GitHub ä»“åº“](https://github.com/psmarter/PMPP-Learning)ï¼ŒåŒ…å«æ¯ç« çš„ç»ƒä¹ é¢˜è§£ç­”ã€CUDA ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚æ‰€æœ‰ä»£ç éƒ½ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œã€‚

## å¼‚æ„è®¡ç®—é›†ç¾¤æ¶æ„

### ä»€ä¹ˆæ˜¯å¼‚æ„é›†ç¾¤

**å¼‚æ„é›†ç¾¤**ï¼šç”±å¤šä¸ªè®¡ç®—èŠ‚ç‚¹ç»„æˆï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«ï¼š

- CPUï¼ˆä¸»æœºï¼‰
- ä¸€ä¸ªæˆ–å¤šä¸ª GPUï¼ˆåŠ é€Ÿå™¨ï¼‰
- æœ¬åœ°å†…å­˜
- ç½‘ç»œæ¥å£

èŠ‚ç‚¹ä¹‹é—´é€šè¿‡é«˜é€Ÿç½‘ç»œï¼ˆå¦‚ InfiniBandã€NVLinkï¼‰è¿æ¥ã€‚

### å…¸å‹æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è®¡ç®—é›†ç¾¤                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    èŠ‚ç‚¹ 0       â”‚    èŠ‚ç‚¹ 1       â”‚    èŠ‚ç‚¹ N-1         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  CPU    â”‚   â”‚  â”‚  CPU    â”‚   â”‚  â”‚  CPU    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚
â”‚       â”‚        â”‚       â”‚        â”‚       â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”        â”‚
â”‚  â”‚GPU0â”‚GPU1â”‚   â”‚  â”‚GPU0â”‚GPU1â”‚   â”‚  â”‚GPU0â”‚GPU1â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    é«˜é€Ÿç½‘ç»œ
```

### ç¼–ç¨‹æŒ‘æˆ˜

1. **åˆ†å¸ƒå¼å†…å­˜**ï¼šæ¯ä¸ªèŠ‚ç‚¹æœ‰ç‹¬ç«‹çš„å†…å­˜ç©ºé—´ï¼Œä¸èƒ½ç›´æ¥è®¿é—®
2. **æ•°æ®é€šä¿¡**ï¼šéœ€è¦æ˜¾å¼åœ°åœ¨èŠ‚ç‚¹é—´ä¼ é€’æ•°æ®
3. **åŒæ­¥åè°ƒ**ï¼šå¤šä¸ªè¿›ç¨‹éœ€è¦åè°ƒå·¥ä½œ
4. **æ•…éšœå®¹é”™**ï¼šå•ä¸ªèŠ‚ç‚¹æ•…éšœä¸åº”å¯¼è‡´æ•´ä¸ªè®¡ç®—å´©æºƒ

## MPI åŸºç¡€

### ä»€ä¹ˆæ˜¯ MPI

**MPIï¼ˆMessage Passing Interfaceï¼‰**ï¼šä¸€ç§æ ‡å‡†åŒ–çš„æ¶ˆæ¯ä¼ é€’ç¼–ç¨‹æ¨¡å‹ã€‚

- å®šä¹‰äº†è¿›ç¨‹é—´é€šä¿¡çš„ API
- æ”¯æŒç‚¹å¯¹ç‚¹é€šä¿¡å’Œé›†åˆé€šä¿¡
- ä¸ç¡¬ä»¶æ— å…³ï¼Œå¯ç§»æ¤æ€§å¥½

å¸¸è§å®ç°ï¼šOpenMPIã€MPICHã€Intel MPIã€‚

### åŸºæœ¬æ¦‚å¿µ

**è¿›ç¨‹ï¼ˆProcessï¼‰**ï¼šMPI ç¨‹åºçš„åŸºæœ¬æ‰§è¡Œå•å…ƒã€‚æ¯ä¸ªè¿›ç¨‹æœ‰ï¼š

- å”¯ä¸€çš„**ç§©ï¼ˆRankï¼‰**ï¼š0 åˆ° N-1
- ç‹¬ç«‹çš„åœ°å€ç©ºé—´
- å¯ä»¥è¿è¡Œåœ¨ä¸åŒçš„ç‰©ç†èŠ‚ç‚¹ä¸Š

**é€šä¿¡å­ï¼ˆCommunicatorï¼‰**ï¼šå®šä¹‰å‚ä¸é€šä¿¡çš„è¿›ç¨‹ç»„ã€‚

- `MPI_COMM_WORLD`ï¼šåŒ…å«æ‰€æœ‰è¿›ç¨‹çš„é»˜è®¤é€šä¿¡å­

### MPI ç¨‹åºéª¨æ¶

```c
#include <mpi.h>

int main(int argc, char *argv[]) {
    int rank, size;
    
    // åˆå§‹åŒ– MPI
    MPI_Init(&argc, &argv);
    
    // è·å–å½“å‰è¿›ç¨‹çš„ç§©
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    
    // è·å–è¿›ç¨‹æ€»æ•°
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    
    printf("è¿›ç¨‹ %d / %d\n", rank, size);
    
    // è®¡ç®—å’Œé€šä¿¡...
    
    // ç»“æŸ MPI
    MPI_Finalize();
    return 0;
}
```

**è¿è¡Œæ–¹å¼**ï¼š

```bash
mpirun -np 4 ./my_program   # å¯åŠ¨ 4 ä¸ªè¿›ç¨‹
```

## ç‚¹å¯¹ç‚¹é€šä¿¡

### åŸºæœ¬å‘é€å’Œæ¥æ”¶

```c
// å‘é€
MPI_Send(
    void *buf,          // å‘é€ç¼“å†²åŒº
    int count,          // å…ƒç´ ä¸ªæ•°
    MPI_Datatype dtype, // æ•°æ®ç±»å‹ï¼ˆMPI_FLOAT, MPI_INT ç­‰ï¼‰
    int dest,           // ç›®æ ‡è¿›ç¨‹ç§©
    int tag,            // æ¶ˆæ¯æ ‡ç­¾
    MPI_Comm comm       // é€šä¿¡å­
);

// æ¥æ”¶
MPI_Recv(
    void *buf,          // æ¥æ”¶ç¼“å†²åŒº
    int count,          // æœ€å¤§å…ƒç´ ä¸ªæ•°
    MPI_Datatype dtype, // æ•°æ®ç±»å‹
    int source,         // æºè¿›ç¨‹ç§©
    int tag,            // æ¶ˆæ¯æ ‡ç­¾
    MPI_Comm comm,      // é€šä¿¡å­
    MPI_Status *status  // çŠ¶æ€ä¿¡æ¯
);
```

### é˜»å¡ä¸éé˜»å¡

**é˜»å¡é€šä¿¡**ï¼šå‡½æ•°è¿”å›æ—¶ï¼Œæ“ä½œå·²å®Œæˆæˆ–ç¼“å†²åŒºå¯å®‰å…¨å¤ç”¨ã€‚

- `MPI_Send`ï¼šå¯èƒ½é˜»å¡ç›´åˆ°æ¥æ”¶æ–¹å‡†å¤‡å¥½ï¼ˆå–å†³äºå®ç°ï¼‰
- `MPI_Recv`ï¼šé˜»å¡ç›´åˆ°æ¶ˆæ¯åˆ°è¾¾

**éé˜»å¡é€šä¿¡**ï¼šå‡½æ•°ç«‹å³è¿”å›ï¼Œåç»­æ£€æŸ¥å®ŒæˆçŠ¶æ€ã€‚

```c
MPI_Request request;

// éé˜»å¡å‘é€
MPI_Isend(buf, count, dtype, dest, tag, comm, &request);

// åšå…¶ä»–äº‹æƒ…...

// ç­‰å¾…å®Œæˆ
MPI_Wait(&request, &status);
```

### Send-Receive ç»„åˆ

é¿å…æ­»é”çš„å¸¸ç”¨æ¨¡å¼ï¼š

```c
MPI_Sendrecv(
    send_buf, send_count, send_type, dest, send_tag,
    recv_buf, recv_count, recv_type, source, recv_tag,
    comm, &status
);
```

åŒæ—¶å‘é€å’Œæ¥æ”¶ï¼Œç³»ç»Ÿè‡ªåŠ¨å¤„ç†é¡ºåºã€‚

## MPI + CUDA ç¼–ç¨‹

### åŸºæœ¬ç­–ç•¥

æ¯ä¸ª MPI è¿›ç¨‹ç®¡ç†ä¸€ä¸ªæˆ–å¤šä¸ª GPUï¼š

```c
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);

// æ¯ä¸ªè¿›ç¨‹é€‰æ‹©ä¸åŒçš„ GPU
int num_devices;
cudaGetDeviceCount(&num_devices);
cudaSetDevice(rank % num_devices);
```

### æ•°æ®æµæ¨¡å¼

å…¸å‹çš„ MPI + CUDA è®¡ç®—æµç¨‹ï¼š

```
1. MPI è¿›ç¨‹æ¥æ”¶è¾“å…¥æ•°æ®ï¼ˆä¸»æœºå†…å­˜ï¼‰
2. å¤åˆ¶æ•°æ®åˆ° GPUï¼ˆcudaMemcpy H2Dï¼‰
3. GPU è®¡ç®—ï¼ˆkernelï¼‰
4. å¤åˆ¶ç»“æœåˆ°ä¸»æœºï¼ˆcudaMemcpy D2Hï¼‰
5. MPI è¿›ç¨‹å‘é€ç»“æœ
```

### ç¤ºä¾‹ï¼šåˆ†å¸ƒå¼å‘é‡åŠ æ³•

```c
void distributed_vector_add(float *a, float *b, float *c, int n, int rank, int size) {
    int local_n = n / size;
    int start = rank * local_n;
    
    // åˆ†é… GPU å†…å­˜
    float *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, local_n * sizeof(float));
    cudaMalloc(&d_b, local_n * sizeof(float));
    cudaMalloc(&d_c, local_n * sizeof(float));
    
    // å¤åˆ¶æœ¬åœ°æ•°æ®åˆ° GPU
    cudaMemcpy(d_a, a + start, local_n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b + start, local_n * sizeof(float), cudaMemcpyHostToDevice);
    
    // GPU è®¡ç®—
    int block_size = 256;
    int grid_size = (local_n + block_size - 1) / block_size;
    vector_add<<<grid_size, block_size>>>(d_a, d_b, d_c, local_n);
    
    // å¤åˆ¶ç»“æœå›ä¸»æœº
    cudaMemcpy(c + start, d_c, local_n * sizeof(float), cudaMemcpyDeviceToHost);
    
    // æ”¶é›†æ‰€æœ‰ç»“æœåˆ°è¿›ç¨‹ 0
    MPI_Gather(c + start, local_n, MPI_FLOAT,
               c, local_n, MPI_FLOAT,
               0, MPI_COMM_WORLD);
    
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);
}
```

## Halo äº¤æ¢ä¸è¾¹ç•Œé€šä¿¡

### ä»€ä¹ˆæ˜¯ Halo

åœ¨æ¨¡æ¿è®¡ç®—ï¼ˆstencilï¼‰ã€æœ‰é™å·®åˆ†ç­‰åº”ç”¨ä¸­ï¼Œæ¯ä¸ªç‚¹çš„è®¡ç®—ä¾èµ–äºé‚»è¿‘ç‚¹ã€‚

å½“æ•°æ®åˆ†å¸ƒåœ¨å¤šä¸ªè¿›ç¨‹æ—¶ï¼Œè¾¹ç•Œç‚¹çš„è®¡ç®—éœ€è¦**ç›¸é‚»è¿›ç¨‹çš„æ•°æ®**ã€‚

**Haloï¼ˆå…‰æ™•/å¹½çµåŒºåŸŸï¼‰**ï¼šå­˜å‚¨æ¥è‡ªé‚»å±…è¿›ç¨‹çš„è¾¹ç•Œæ•°æ®ã€‚

```
è¿›ç¨‹ 0 çš„æ•°æ®        è¿›ç¨‹ 1 çš„æ•°æ®
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚    â”‚              â”‚
â”‚   å†…éƒ¨åŒºåŸŸ   â”‚    â”‚   å†…éƒ¨åŒºåŸŸ   â”‚
â”‚              â”‚    â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å³è¾¹ç•Œ â†’â†’â†’  â”‚ â†”  â”‚ â†â†â† å·¦ Halo â”‚
â”‚ (å‘é€ç»™ P1)  â”‚    â”‚ (æ¥è‡ª P0)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3D æ¨¡æ¿è®¡ç®—ç¤ºä¾‹

ä»¥ 25 ç‚¹æ¨¡æ¿ä¸ºä¾‹ï¼ˆæ¯ä¸ªæ–¹å‘å»¶ä¼¸ 4 ä¸ªç‚¹ï¼‰ï¼š

```c
// è®¡ç®—æ¯ä¸ªè¿›ç¨‹éœ€è¦å¤šå°‘ Halo ç‚¹
int halo_size = 4;  // æ¯ä¾§ 4 å±‚
int num_halo_points = dimx * dimy * halo_size;

// åˆ†é…åŒ…å« Halo çš„æ•°æ®
int total_z = local_dimz + 2 * halo_size;
float *data = malloc(dimx * dimy * total_z * sizeof(float));
```

### Halo äº¤æ¢å®ç°

```c
void exchange_halos(float *data, int dimx, int dimy, int dimz,
                     int left_neighbor, int right_neighbor) {
    int halo_size = 4;
    int num_halo_points = dimx * dimy * halo_size;
    
    float *left_send = data + num_halo_points;  // å·¦è¾¹ç•Œæ•°æ®
    float *right_send = data + dimx * dimy * (dimz - halo_size);  // å³è¾¹ç•Œæ•°æ®
    float *left_recv = data;  // å·¦ Halo æ¥æ”¶åŒº
    float *right_recv = data + dimx * dimy * (dimz + halo_size);  // å³ Halo æ¥æ”¶åŒº
    
    MPI_Status status;
    
    // å‘é€åˆ°å·¦é‚»å±…ï¼Œä»å³é‚»å±…æ¥æ”¶
    MPI_Sendrecv(left_send, num_halo_points, MPI_FLOAT, left_neighbor, 0,
                 right_recv, num_halo_points, MPI_FLOAT, right_neighbor, 0,
                 MPI_COMM_WORLD, &status);
    
    // å‘é€åˆ°å³é‚»å±…ï¼Œä»å·¦é‚»å±…æ¥æ”¶
    MPI_Sendrecv(right_send, num_halo_points, MPI_FLOAT, right_neighbor, 1,
                 left_recv, num_halo_points, MPI_FLOAT, left_neighbor, 1,
                 MPI_COMM_WORLD, &status);
}
```

## è®¡ç®—ä¸é€šä¿¡é‡å 

### é—®é¢˜

é€šä¿¡éœ€è¦æ—¶é—´ï¼Œå¦‚æœå…ˆè®¡ç®—å®Œå†é€šä¿¡ï¼ŒGPU ä¼šç©ºé—²ç­‰å¾…ã€‚

### è§£å†³æ–¹æ¡ˆ

**æ€è·¯**ï¼šé‡å è®¡ç®—ä¸é€šä¿¡ã€‚

1. å…ˆè®¡ç®—è¾¹ç•ŒåŒºåŸŸï¼ˆé€šä¿¡éœ€è¦çš„æ•°æ®ï¼‰
2. è¾¹ç•Œè®¡ç®—å®Œæˆåï¼Œå¼€å§‹é€šä¿¡
3. åŒæ—¶è®¡ç®—å†…éƒ¨åŒºåŸŸ
4. é€šä¿¡å®Œæˆåï¼Œæ‰€æœ‰è®¡ç®—éƒ½å®Œæˆäº†

### CUDA æµå®ç°

```cuda
cudaStream_t stream_boundary, stream_internal;
cudaStreamCreate(&stream_boundary);
cudaStreamCreate(&stream_internal);

// é˜¶æ®µ 1ï¼šè®¡ç®—è¾¹ç•Œï¼ˆåœ¨ stream_boundary ä¸­ï¼‰
stencil_kernel<<<grid_boundary, block, 0, stream_boundary>>>(
    d_output + left_offset, d_input + left_offset, dimx, dimy, 12);
stencil_kernel<<<grid_boundary, block, 0, stream_boundary>>>(
    d_output + right_offset, d_input + right_offset, dimx, dimy, 12);

// é˜¶æ®µ 2ï¼šåŒæ—¶è¿›è¡Œâ€”â€”
// 2a: è®¡ç®—å†…éƒ¨åŒºåŸŸï¼ˆåœ¨ stream_internal ä¸­ï¼‰
stencil_kernel<<<grid_internal, block, 0, stream_internal>>>(
    d_output + internal_offset, d_input + internal_offset, dimx, dimy, dimz - 8);

// 2b: å¤åˆ¶è¾¹ç•Œåˆ°ä¸»æœºï¼Œå‡†å¤‡å‘é€
cudaMemcpyAsync(h_left_boundary, d_output + boundary_left,
                num_halo_bytes, cudaMemcpyDeviceToHost, stream_boundary);
cudaMemcpyAsync(h_right_boundary, d_output + boundary_right,
                num_halo_bytes, cudaMemcpyDeviceToHost, stream_boundary);

// ç­‰å¾…è¾¹ç•Œå¤åˆ¶å®Œæˆ
cudaStreamSynchronize(stream_boundary);

// é˜¶æ®µ 3ï¼šMPI é€šä¿¡
MPI_Sendrecv(h_left_boundary, num_halo_points, MPI_FLOAT, left_neighbor, 0,
             h_right_halo, num_halo_points, MPI_FLOAT, right_neighbor, 0,
             MPI_COMM_WORLD, &status);
MPI_Sendrecv(h_right_boundary, num_halo_points, MPI_FLOAT, right_neighbor, 1,
             h_left_halo, num_halo_points, MPI_FLOAT, left_neighbor, 1,
             MPI_COMM_WORLD, &status);

// å¤åˆ¶ Halo å› GPU
cudaMemcpyAsync(d_output + left_halo_offset, h_left_halo,
                num_halo_bytes, cudaMemcpyHostToDevice, stream_boundary);
cudaMemcpyAsync(d_output + right_halo_offset, h_right_halo,
                num_halo_bytes, cudaMemcpyHostToDevice, stream_boundary);

// ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
cudaDeviceSynchronize();
```

### æ—¶é—´çº¿åˆ†æ

```
                    æ—¶é—´ â†’
stream_boundary:  [è¾¹ç•Œè®¡ç®—][D2H][       ][H2D]
stream_internal:  [           å†…éƒ¨è®¡ç®—          ]
MPI é€šä¿¡:         [      ][Sendrecv][      ]
                           â†‘
                     é‡å åŒºåŸŸï¼šå†…éƒ¨è®¡ç®—
                     ä¸ MPI é€šä¿¡åŒæ—¶è¿›è¡Œ
```

## CUDA-Aware MPI

### ä¼ ç»Ÿæ–¹å¼çš„é—®é¢˜

```c
// ä¼ ç»Ÿæ–¹å¼ï¼šå¿…é¡»ç»è¿‡ä¸»æœºå†…å­˜
cudaMemcpy(h_buf, d_buf, size, cudaMemcpyDeviceToHost);  // GPU â†’ CPU
MPI_Send(h_buf, count, MPI_FLOAT, dest, tag, comm);       // CPU â†’ ç½‘ç»œ
// æ¥æ”¶ç«¯
MPI_Recv(h_buf, count, MPI_FLOAT, src, tag, comm, &status);  // ç½‘ç»œ â†’ CPU  
cudaMemcpy(d_buf, h_buf, size, cudaMemcpyHostToDevice);      // CPU â†’ GPU
```

**é—®é¢˜**ï¼šé¢å¤–çš„å†…å­˜æ‹·è´å¼€é”€ã€‚

### CUDA-Aware MPI

**CUDA-Aware MPI**ï¼šMPI å®ç°èƒ½ç›´æ¥è¯†åˆ« GPU æŒ‡é’ˆã€‚

```c
// ç›´æ¥ä¼ é€’ GPU æŒ‡é’ˆâ€”â€”ä¸éœ€è¦æ‰‹åŠ¨æ‹·è´
MPI_Send(d_buf, count, MPI_FLOAT, dest, tag, comm);
MPI_Recv(d_buf, count, MPI_FLOAT, src, tag, comm, &status);
```

MPI åº“è‡ªåŠ¨å¤„ç†ï¼š

- é€šè¿‡ GPUDirect RDMA ç›´æ¥ GPU åˆ° GPU ä¼ è¾“
- å¦‚æœä¸æ”¯æŒï¼Œè‡ªåŠ¨å›é€€åˆ°ç»è¿‡ä¸»æœºçš„æ–¹å¼

### ç¯å¢ƒé…ç½®

```bash
# ç¼–è¯‘æ—¶é“¾æ¥ CUDA-Aware MPI
mpicc -o my_prog my_prog.c -I${CUDA_HOME}/include -L${CUDA_HOME}/lib64 -lcudart

# è¿è¡Œå‰è®¾ç½®
export UCX_RNDV_SCHEME=cuda
export UCX_TLS=rc,cuda_copy,cuda_ipc
```

### ä½¿ç”¨ CUDA-Aware MPI æ”¹å†™

```c
// æ— éœ€ host bufferï¼Œç›´æ¥ä½¿ç”¨ device buffer
MPI_Sendrecv(d_output + boundary_left, num_halo_points, MPI_FLOAT, left_neighbor, 0,
             d_output + right_halo_offset, num_halo_points, MPI_FLOAT, right_neighbor, 0,
             MPI_COMM_WORLD, &status);
```

**ä¼˜åŠ¿**ï¼š

- å‡å°‘å†…å­˜æ‹·è´
- æ›´ä½å»¶è¿Ÿ
- ä»£ç æ›´ç®€æ´

## æ•°æ®æœåŠ¡å™¨æ¨¡å¼

### é—®é¢˜

å¤§è§„æ¨¡é›†ç¾¤ä¸­ï¼ŒI/O å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚æ¯ä¸ªè®¡ç®—èŠ‚ç‚¹éƒ½ä»å­˜å‚¨è¯»æ•°æ®ä¼šå¯¼è‡´äº‰ç”¨ã€‚

### è§£å†³æ–¹æ¡ˆ

**æ•°æ®æœåŠ¡å™¨æ¨¡å¼**ï¼šä¸€ä¸ªè¿›ç¨‹ä¸“é—¨è´Ÿè´£ I/Oï¼Œå…¶ä»–è¿›ç¨‹ä¸“é—¨è®¡ç®—ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ•°æ®æœåŠ¡å™¨ (Rank N-1)                               â”‚
â”‚  - è¯»å–è¾“å…¥æ•°æ®                                      â”‚
â”‚  - åˆ†å‘æ•°æ®ç»™è®¡ç®—èŠ‚ç‚¹                                â”‚
â”‚  - æ”¶é›†è®¡ç®—ç»“æœ                                      â”‚
â”‚  - å†™å…¥è¾“å‡º                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ åˆ†å‘           â†‘ æ”¶é›†
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rank 0  â”‚ Rank 1  â”‚ Rank 2  â”‚   ...   â”‚
â”‚ è®¡ç®—    â”‚ è®¡ç®—    â”‚ è®¡ç®—    â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®ç°

```c
void data_server(int dimx, int dimy, int dimz, int nreps) {
    int np, num_comp_nodes;
    MPI_Comm_size(MPI_COMM_WORLD, &np);
    num_comp_nodes = np - 1;
    
    // åˆ†é…å¹¶åˆå§‹åŒ–æ•°æ®
    float *input = malloc(dimx * dimy * dimz * sizeof(float));
    float *output = malloc(dimx * dimy * dimz * sizeof(float));
    initialize_data(input, dimx, dimy, dimz);
    
    // è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®é‡
    int slice_per_node = dimz / num_comp_nodes;
    int halo_size = 4;
    
    // åˆ†å‘æ•°æ®ç»™è®¡ç®—èŠ‚ç‚¹
    for (int p = 0; p < num_comp_nodes; p++) {
        int start_z = p * slice_per_node - (p > 0 ? halo_size : 0);
        int num_slices = slice_per_node + (p > 0 ? halo_size : 0) 
                                        + (p < num_comp_nodes - 1 ? halo_size : 0);
        int num_points = dimx * dimy * num_slices;
        
        MPI_Send(input + start_z * dimx * dimy, num_points, MPI_FLOAT,
                 p, 0, MPI_COMM_WORLD);
    }
    
    // ç­‰å¾…è®¡ç®—å®Œæˆ
    MPI_Barrier(MPI_COMM_WORLD);
    
    // æ”¶é›†ç»“æœ
    for (int p = 0; p < num_comp_nodes; p++) {
        int offset = p * slice_per_node * dimx * dimy;
        int num_points = slice_per_node * dimx * dimy;
        
        MPI_Recv(output + offset, num_points, MPI_FLOAT,
                 p, DATA_COLLECT, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }
    
    // ä¿å­˜ç»“æœ
    save_output(output, dimx, dimy, dimz);
    
    free(input);
    free(output);
}
```

## å®Œæ•´ç¤ºä¾‹ï¼šMPI + CUDA æ¨¡æ¿è®¡ç®—

### ç¨‹åºç»“æ„

```c
int main(int argc, char *argv[]) {
    int pid, np;
    
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &pid);
    MPI_Comm_size(MPI_COMM_WORLD, &np);
    
    if (pid < np - 1) {
        // è®¡ç®—èŠ‚ç‚¹ï¼šè®¾ç½® GPU å¹¶è®¡ç®—
        int device = pid % num_devices;
        cudaSetDevice(device);
        compute_node_stencil(dimx, dimy, dimz / (np - 1), nreps);
    } else {
        // æ•°æ®æœåŠ¡å™¨ï¼šI/O å’Œæ•°æ®åˆ†å‘
        data_server(dimx, dimy, dimz, nreps);
    }
    
    MPI_Finalize();
    return 0;
}
```

### è®¡ç®—èŠ‚ç‚¹å®ç°

```c
void compute_node_stencil(int dimx, int dimy, int dimz, int nreps) {
    int pid, np;
    MPI_Comm_rank(MPI_COMM_WORLD, &pid);
    MPI_Comm_size(MPI_COMM_WORLD, &np);
    
    int server = np - 1;
    int left_neighbor = (pid > 0) ? (pid - 1) : MPI_PROC_NULL;
    int right_neighbor = (pid < np - 2) ? (pid + 1) : MPI_PROC_NULL;
    
    // åˆ†é…å†…å­˜
    int halo = 4;
    int total_z = dimz + 2 * halo;
    size_t num_bytes = dimx * dimy * total_z * sizeof(float);
    
    float *h_input = malloc(num_bytes);
    float *h_output = malloc(num_bytes);
    float *d_input, *d_output;
    cudaMalloc(&d_input, num_bytes);
    cudaMalloc(&d_output, num_bytes);
    
    // Halo ç¼“å†²åŒºï¼ˆå›ºå®šå†…å­˜ï¼ŒåŠ é€Ÿä¼ è¾“ï¼‰
    float *h_left_boundary, *h_right_boundary;
    float *h_left_halo, *h_right_halo;
    size_t halo_bytes = dimx * dimy * halo * sizeof(float);
    cudaHostAlloc(&h_left_boundary, halo_bytes, cudaHostAllocDefault);
    cudaHostAlloc(&h_right_boundary, halo_bytes, cudaHostAllocDefault);
    cudaHostAlloc(&h_left_halo, halo_bytes, cudaHostAllocDefault);
    cudaHostAlloc(&h_right_halo, halo_bytes, cudaHostAllocDefault);
    
    // åˆ›å»º CUDA æµ
    cudaStream_t stream_boundary, stream_internal;
    cudaStreamCreate(&stream_boundary);
    cudaStreamCreate(&stream_internal);
    
    // ä»æ•°æ®æœåŠ¡å™¨æ¥æ”¶åˆå§‹æ•°æ®
    MPI_Status status;
    MPI_Recv(h_input, dimx * dimy * total_z, MPI_FLOAT,
             server, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
    cudaMemcpy(d_input, h_input, num_bytes, cudaMemcpyHostToDevice);
    
    // è¿­ä»£è®¡ç®—
    for (int iter = 0; iter < nreps; iter++) {
        // é˜¶æ®µ 1ï¼šè¾¹ç•Œè®¡ç®—
        launch_boundary_kernel(d_output, d_input, dimx, dimy, stream_boundary);
        
        // é˜¶æ®µ 2ï¼šå†…éƒ¨è®¡ç®—ï¼ˆä¸é€šä¿¡é‡å ï¼‰
        launch_internal_kernel(d_output, d_input, dimx, dimy, dimz, stream_internal);
        
        // å¤åˆ¶è¾¹ç•Œåˆ°ä¸»æœº
        copy_boundary_to_host(d_output, h_left_boundary, h_right_boundary,
                               dimx, dimy, halo, stream_boundary);
        cudaStreamSynchronize(stream_boundary);
        
        // Halo äº¤æ¢
        MPI_Sendrecv(h_left_boundary, dimx * dimy * halo, MPI_FLOAT, left_neighbor, iter,
                     h_right_halo, dimx * dimy * halo, MPI_FLOAT, right_neighbor, iter,
                     MPI_COMM_WORLD, &status);
        MPI_Sendrecv(h_right_boundary, dimx * dimy * halo, MPI_FLOAT, right_neighbor, iter,
                     h_left_halo, dimx * dimy * halo, MPI_FLOAT, left_neighbor, iter,
                     MPI_COMM_WORLD, &status);
        
        // å¤åˆ¶ Halo å› GPU
        copy_halo_to_device(d_output, h_left_halo, h_right_halo,
                             dimx, dimy, dimz, halo, stream_boundary);
        
        cudaDeviceSynchronize();
        
        // äº¤æ¢è¾“å…¥è¾“å‡ºæŒ‡é’ˆ
        float *temp = d_output;
        d_output = d_input;
        d_input = temp;
    }
    
    // å‘é€ç»“æœç»™æ•°æ®æœåŠ¡å™¨
    cudaMemcpy(h_output, d_input, num_bytes, cudaMemcpyDeviceToHost);
    MPI_Send(h_output + dimx * dimy * halo, dimx * dimy * dimz, MPI_FLOAT,
             server, DATA_COLLECT, MPI_COMM_WORLD);
    
    // æ¸…ç†
    free(h_input);
    free(h_output);
    cudaFreeHost(h_left_boundary);
    cudaFreeHost(h_right_boundary);
    cudaFreeHost(h_left_halo);
    cudaFreeHost(h_right_halo);
    cudaFree(d_input);
    cudaFree(d_output);
    cudaStreamDestroy(stream_boundary);
    cudaStreamDestroy(stream_internal);
}
```

## æ€§èƒ½ä¼˜åŒ–

### é€šä¿¡ä¼˜åŒ–

| æŠ€æœ¯           | æè¿°                              | æ•ˆæœ              |
| -------------- | --------------------------------- | ----------------- |
| éé˜»å¡é€šä¿¡     | ä½¿ç”¨ `MPI_Isend`/`MPI_Irecv`      | é‡å é€šä¿¡ä¸è®¡ç®—    |
| é›†åˆé€šä¿¡       | ä½¿ç”¨ `MPI_Allreduce` è€Œéå¾ªç¯ P2P | åˆ©ç”¨ä¼˜åŒ–çš„ç®—æ³•    |
| CUDA-Aware MPI | ç›´æ¥ä¼ é€’ GPU æŒ‡é’ˆ                 | å‡å°‘å†…å­˜æ‹·è´      |
| å›ºå®šå†…å­˜       | `cudaHostAlloc`                   | åŠ é€Ÿ H2D/D2H ä¼ è¾“ |

### è´Ÿè½½å‡è¡¡

ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹çš„å·¥ä½œé‡å¤§è‡´ç›¸ç­‰ï¼š

```c
// å¤„ç†ä¸èƒ½æ•´é™¤çš„æƒ…å†µ
int base_slices = dimz / num_nodes;
int remainder = dimz % num_nodes;

for (int p = 0; p < num_nodes; p++) {
    int slices = base_slices + (p < remainder ? 1 : 0);
    // åˆ†é… slices ç»™èŠ‚ç‚¹ p
}
```

### å¯æ‰©å±•æ€§åˆ†æ

å¯¹äº 25 ç‚¹æ¨¡æ¿è®¡ç®—ï¼š

| èŠ‚ç‚¹æ•° | é€šä¿¡é‡ï¼ˆæ¯èŠ‚ç‚¹ï¼‰ | è®¡ç®—é‡ï¼ˆæ¯èŠ‚ç‚¹ï¼‰ | è®¡ç®—/é€šä¿¡æ¯” |
| ------ | ---------------- | ---------------- | ----------- |
| 16     | 2Ã—64Ã—64Ã—4 = 32K  | 64Ã—64Ã—128 = 512K | 16:1        |
| 64     | 2Ã—64Ã—64Ã—4 = 32K  | 64Ã—64Ã—32 = 128K  | 4:1         |
| 256    | 2Ã—64Ã—64Ã—4 = 32K  | 64Ã—64Ã—8 = 32K    | 1:1         |

**è§‚å¯Ÿ**ï¼šèŠ‚ç‚¹è¶Šå¤šï¼Œé€šä¿¡å¼€é”€å æ¯”è¶Šé«˜ã€‚è¿™æ˜¯**å¼ºæ‰©å±•**çš„å…¸å‹ç‰¹å¾ã€‚

## å¸¸è§é—®é¢˜ä¸è§£å†³

### æ­»é”

**åŸå› **ï¼šæ‰€æœ‰è¿›ç¨‹éƒ½åœ¨ç­‰å¾…æ¥æ”¶ï¼Œæ²¡æœ‰è¿›ç¨‹å‘é€ã€‚

```c
// é”™è¯¯ç¤ºä¾‹â€”â€”ä¼šæ­»é”ï¼
if (rank == 0) {
    MPI_Recv(..., 1, ...);  // ç­‰å¾… rank 1
    MPI_Send(..., 1, ...);
} else {
    MPI_Recv(..., 0, ...);  // ç­‰å¾… rank 0
    MPI_Send(..., 0, ...);
}
```

**è§£å†³**ï¼šä½¿ç”¨ `MPI_Sendrecv` æˆ–éé˜»å¡é€šä¿¡ã€‚

### GPU å†…å­˜ä¸è¶³

**åŸå› **ï¼šæ¯ä¸ªèŠ‚ç‚¹åˆ†é…çš„æ•°æ®å¤ªå¤šã€‚

**è§£å†³**ï¼š

- å¢åŠ èŠ‚ç‚¹æ•°
- ä½¿ç”¨ç»Ÿä¸€å†…å­˜è‡ªåŠ¨ç®¡ç†
- åˆ†æ‰¹å¤„ç†

### æ€§èƒ½ä¸ä½³

**è¯Šæ–­**ï¼šä½¿ç”¨ Nsight Systems åˆ†æ MPI + CUDA ç¨‹åºã€‚

```bash
nsys profile --trace=cuda,mpi mpirun -np 4 ./my_program
```

æŸ¥çœ‹æ˜¯å¦æœ‰ï¼š

- è¿‡é•¿çš„ MPI ç­‰å¾…æ—¶é—´
- æœªé‡å çš„è®¡ç®—å’Œé€šä¿¡
- GPU ç©ºé—²æ—¶é—´

## å°ç»“

ç¬¬äºŒåç« æ‰©å±•äº†å¹¶è¡Œç¼–ç¨‹çš„è§†é‡ï¼Œä»å• GPU æ‰©å±•åˆ°å¤šèŠ‚ç‚¹é›†ç¾¤ï¼š

**å¼‚æ„é›†ç¾¤æ¶æ„**ï¼šæ¯ä¸ªèŠ‚ç‚¹åŒ…å« CPU å’Œ GPUï¼ŒèŠ‚ç‚¹é—´é€šè¿‡ç½‘ç»œè¿æ¥ã€‚åˆ†å¸ƒå¼å†…å­˜æ¨¡å‹è¦æ±‚æ˜¾å¼é€šä¿¡ã€‚

**MPI åŸºç¡€**ï¼šæ¶ˆæ¯ä¼ é€’ç¼–ç¨‹æ¨¡å‹ã€‚è¿›ç¨‹é€šè¿‡å‘é€/æ¥æ”¶æ¶ˆæ¯é€šä¿¡ã€‚ç‚¹å¯¹ç‚¹é€šä¿¡ï¼ˆSend/Recvï¼‰å’Œé›†åˆé€šä¿¡ï¼ˆBroadcast/Reduceï¼‰ã€‚

**MPI + CUDA**ï¼šæ¯ä¸ª MPI è¿›ç¨‹ç®¡ç†ä¸€ä¸ªæˆ–å¤šä¸ª GPUã€‚æ•°æ®åœ¨ä¸»æœºå†…å­˜å’Œ GPU å†…å­˜ä¹‹é—´ä¼ è¾“ï¼Œåœ¨è¿›ç¨‹é—´é€šè¿‡ MPI ä¼ è¾“ã€‚

**Halo äº¤æ¢**ï¼šæ¨¡æ¿è®¡ç®—ä¸­ï¼Œè¾¹ç•Œæ•°æ®éœ€è¦ä¸é‚»å±…è¿›ç¨‹äº¤æ¢ã€‚ä½¿ç”¨ Sendrecv é¿å…æ­»é”ã€‚

**è®¡ç®—ä¸é€šä¿¡é‡å **ï¼šåˆ©ç”¨ CUDA æµï¼Œè¾¹ç•Œè®¡ç®—å®Œæˆåç«‹å³å¼€å§‹é€šä¿¡ï¼ŒåŒæ—¶è¿›è¡Œå†…éƒ¨è®¡ç®—ã€‚æ˜¾è‘—å‡å°‘æ€»æ‰§è¡Œæ—¶é—´ã€‚

**CUDA-Aware MPI**ï¼šMPI åº“ç›´æ¥æ¥å— GPU æŒ‡é’ˆï¼Œåˆ©ç”¨ GPUDirect æŠ€æœ¯å‡å°‘å†…å­˜æ‹·è´ã€‚

**æ•°æ®æœåŠ¡å™¨æ¨¡å¼**ï¼šä¸€ä¸ªè¿›ç¨‹ä¸“é—¨è´Ÿè´£ I/Oï¼Œå‡å°‘å­˜å‚¨äº‰ç”¨ã€‚

æŒæ¡ MPI + CUDA ç¼–ç¨‹ï¼Œä½ å°±èƒ½ç¼–å†™å¯æ‰©å±•åˆ°æ•°åƒ GPU çš„åº”ç”¨ç¨‹åºâ€”â€”è¿™æ˜¯å½“ä»Š AI è®­ç»ƒã€ç§‘å­¦è®¡ç®—ã€å¤©æ°”é¢„æŠ¥ç­‰é¢†åŸŸçš„æ ¸å¿ƒæŠ€æœ¯ã€‚

## ğŸš€ ä¸‹ä¸€æ­¥

- æ­å»ºä¸€ä¸ªç®€å•çš„å¤šèŠ‚ç‚¹ GPU é›†ç¾¤ç¯å¢ƒï¼Œé…ç½® MPI å’Œ CUDA-Aware MPI
- å®ç°ä¸€ä¸ªåˆ†å¸ƒå¼çŸ©é˜µä¹˜æ³•ï¼Œå­¦ä¹ æ•°æ®åˆ†å‰²å’Œç»“æœæ”¶é›†
- æŒæ¡ Halo äº¤æ¢æ¨¡å¼ï¼Œå®ç°åˆ†å¸ƒå¼æ¨¡æ¿è®¡ç®—ï¼ˆå¦‚çƒ­ä¼ å¯¼æ–¹ç¨‹ï¼‰
- å­¦ä¹ é›†åˆé€šä¿¡æ“ä½œï¼šAllreduceã€Allgatherã€Alltoall
- æ¢ç´¢æ€§èƒ½åˆ†æå·¥å…·ï¼šNsight Systems åˆ†æ MPI + CUDA ç¨‹åºçš„æ€§èƒ½ç“¶é¢ˆ
- äº†è§£ç°ä»£ HPC æ¡†æ¶ï¼šNCCLï¼ˆå¤š GPU é€šä¿¡ï¼‰ã€Horovodï¼ˆåˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ï¼‰

---

## ğŸ“š å‚è€ƒèµ„æ–™

- PMPP ç¬¬å››ç‰ˆ Chapter 20
- [ç¬¬äºŒåç« ï¼šå¼‚æ„è®¡ç®—é›†ç¾¤ç¼–ç¨‹](https://smarter.xin/posts/pmmpp-chapter20-heterogeneous-clusters/)
- Hwu, W., Kirk, D., & El Hajj, I. (2022). *Programming Massively Parallel Processors: A Hands-on Approach* (4th Edition). Morgan Kaufmann.
- MPI Forum. *MPI: A Message-Passing Interface Standard*. <https://www.mpi-forum.org/>
- NVIDIA. *CUDA-Aware MPI*. <https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/>

**å­¦ä¹ æ„‰å¿«ï¼** ğŸ“

---

> **æœ¬æ–‡ GitHub ä»“åº“**: [https://github.com/psmarter/PMPP-Learning](https://github.com/psmarter/PMPP-Learning)
