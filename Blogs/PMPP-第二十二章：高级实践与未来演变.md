---
title: PMPP-ç¬¬äºŒåäºŒç« ï¼šé«˜çº§å®è·µä¸æœªæ¥æ¼”å˜
date: 2026-01-24 16:56:01
tags:
  - CUDA
  - GPUç¼–ç¨‹
  - å¹¶è¡Œè®¡ç®—
  - PMPP
  - æ€§èƒ½ä¼˜åŒ–
  - æœªæ¥è¶‹åŠ¿
  - æœ€ä½³å®è·µ
categories: çŸ¥è¯†åˆ†äº«
cover: /img/PMPP.jpg
---

## å‰è¨€

ç¬¬äºŒåä¸€ç« ä»‹ç»äº†åŠ¨æ€å¹¶è¡Œæ€§ã€‚ä½œä¸ºå…¨ä¹¦çš„æœ€åä¸€ç« ï¼Œç¬¬äºŒåäºŒç« æ€»ç»“äº†**é«˜çº§å®è·µï¼ˆAdvanced Practicesï¼‰**å¹¶å±•æœ›äº†**æœªæ¥æ¼”å˜ï¼ˆFuture Evolutionï¼‰**ã€‚æœ¬ç« ä¸å†å¼•å…¥æ–°çš„ç¼–ç¨‹æŠ€æœ¯ï¼Œè€Œæ˜¯è®¨è®ºå¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨å‰é¢å­¦åˆ°çš„çŸ¥è¯†ï¼Œä»¥åŠ GPU è®¡ç®—é¢†åŸŸçš„å‘å±•è¶‹åŠ¿ã€‚æŒæ¡è¿™äº›å†…å®¹ï¼Œå°†æœ‰åŠ©äºåœ¨å¿«é€Ÿå˜åŒ–çš„æŠ€æœ¯ç¯å¢ƒä¸­æŒç»­æˆé•¿ã€‚

> **ğŸ“¦ é…å¥—èµ„æº**ï¼šæœ¬ç³»åˆ—æ–‡ç« é…æœ‰å®Œæ•´çš„ [GitHub ä»“åº“](https://github.com/psmarter/PMPP-Learning)ï¼ŒåŒ…å«æ¯ç« çš„ç»ƒä¹ é¢˜è§£ç­”ã€CUDA ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚æ‰€æœ‰ä»£ç éƒ½ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œã€‚

## æ€§èƒ½ä¼˜åŒ–æ–¹æ³•è®º

### æ€§èƒ½åˆ†æé©±åŠ¨

**ä¸è¦çŒœæµ‹ï¼Œè¦æµ‹é‡**ã€‚

ä¼˜åŒ–å‰å¿…é¡»å…ˆæ‰¾åˆ°ç“¶é¢ˆï¼š

1. ä½¿ç”¨ Nsight Systems åˆ†ææ•´ä½“æµç¨‹
2. ä½¿ç”¨ Nsight Compute åˆ†æå•ä¸ª kernel
3. æ ¹æ®æ•°æ®å†³å®šä¼˜åŒ–æ–¹å‘

### Roofline æ¨¡å‹å›é¡¾

```
æ€§èƒ½ä¸Šé™ = min(å³°å€¼ç®—åŠ›, å¸¦å®½ Ã— ç®—æœ¯å¼ºåº¦)
```

**ç®—æœ¯å¼ºåº¦** = FLOP / Byte

| ç®—æœ¯å¼ºåº¦ | ç“¶é¢ˆç±»å‹ | ä¼˜åŒ–æ–¹å‘                 |
| -------- | -------- | ------------------------ |
| < 10     | å†…å­˜å—é™ | æé«˜æ•°æ®å¤ç”¨ã€å‡å°‘è®¿å­˜   |
| > 10     | è®¡ç®—å—é™ | ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€å‡å°‘æŒ‡ä»¤æ•° |

### ä¼˜åŒ–å±‚æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬äº”å±‚ï¼šç®—æ³•é€‰æ‹©                         â”‚
â”‚   é€‰æ‹©æ›´é«˜æ•ˆçš„ç®—æ³•ï¼ˆæ”¶ç›Šæœ€å¤§ï¼‰            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¬¬å››å±‚ï¼šæ•°æ®å¸ƒå±€                         â”‚
â”‚   SoA vs AoSã€å¯¹é½ã€Padding             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¬¬ä¸‰å±‚ï¼šå¹¶è¡Œç­–ç•¥                         â”‚
â”‚   ä»»åŠ¡åˆ†è§£ã€è´Ÿè½½å‡è¡¡ã€çº¿ç¨‹æ˜ å°„            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¬¬äºŒå±‚ï¼šå†…å­˜å±‚æ¬¡                         â”‚
â”‚   å…±äº«å†…å­˜ã€å¯„å­˜å™¨ã€ç¼“å­˜åˆ©ç”¨              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¬¬ä¸€å±‚ï¼šæŒ‡ä»¤çº§                           â”‚
â”‚   å¾ªç¯å±•å¼€ã€å‘é‡åŒ–ã€å¿«é€Ÿæ•°å­¦å‡½æ•°          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»ä¸Šå¾€ä¸‹ä¼˜åŒ–**ï¼šé«˜å±‚ä¼˜åŒ–çš„æ”¶ç›Šé€šå¸¸è¿œå¤§äºä½å±‚ã€‚

## å¸¸è§ä¼˜åŒ–æŠ€æœ¯æ€»ç»“

### å†…å­˜ä¼˜åŒ–

| æŠ€æœ¯          | é€‚ç”¨åœºæ™¯     | æ•ˆæœ         |
| ------------- | ------------ | ------------ |
| åˆå¹¶è®¿é—®      | å…¨å±€å†…å­˜è®¿é—® | å‡å°‘å†…å­˜äº‹åŠ¡ |
| å…±äº«å†…å­˜      | æ•°æ®å¤ç”¨     | å‡å°‘å…¨å±€è®¿é—® |
| å¸¸é‡å†…å­˜      | åªè¯»å¹¿æ’­æ•°æ® | åˆ©ç”¨ç¼“å­˜     |
| çº¹ç†å†…å­˜      | 2D å±€éƒ¨æ€§    | ç¡¬ä»¶æ’å€¼     |
| å¯„å­˜å™¨ Tiling | çŸ©é˜µè¿ç®—     | æœ€å¤§åŒ–å¤ç”¨   |

### è®¡ç®—ä¼˜åŒ–

| æŠ€æœ¯         | é€‚ç”¨åœºæ™¯       | æ•ˆæœ         |
| ------------ | -------------- | ------------ |
| å¾ªç¯å±•å¼€     | å›ºå®šè¿­ä»£å¾ªç¯   | å‡å°‘æŒ‡ä»¤å¼€é”€ |
| çº¿ç¨‹ç²—åŒ–     | æ¯çº¿ç¨‹å·¥ä½œå¤ªå°‘ | å‡å°‘è°ƒåº¦å¼€é”€ |
| å¿«é€Ÿæ•°å­¦     | ç²¾åº¦è¦æ±‚ä¸é«˜   | å‡å°‘æ—¶é’Ÿå‘¨æœŸ |
| Warp Shuffle | Warp å†…é€šä¿¡    | é¿å…å…±äº«å†…å­˜ |

### å¹¶è¡Œåº¦ä¼˜åŒ–

| æŠ€æœ¯       | é€‚ç”¨åœºæ™¯    | æ•ˆæœ          |
| ---------- | ----------- | ------------- |
| å¢åŠ çº¿ç¨‹æ•° | éšè—å»¶è¿Ÿ    | æé«˜å ç”¨ç‡    |
| åŠ¨æ€å¹¶è¡Œ   | é€’å½’/è‡ªé€‚åº” | å‡å°‘ CPU å‚ä¸ |
| CUDA æµ    | å¤šä»»åŠ¡é‡å   | éšè—ä¼ è¾“å»¶è¿Ÿ  |

## ä»£ç å¯ç§»æ¤æ€§

### è·¨ GPU æ¶æ„

ä¸åŒ GPU æ¶æ„æœ‰ä¸åŒç‰¹æ€§ï¼š

| æ¶æ„            | ç‰¹ç‚¹                      |
| --------------- | ------------------------- |
| Kepler (sm_35)  | åŠ¨æ€å¹¶è¡Œé¦–æ¬¡æ”¯æŒ          |
| Maxwell (sm_50) | æ”¹è¿›çš„å…±äº«å†…å­˜            |
| Pascal (sm_60)  | ç»Ÿä¸€å†…å­˜æ”¹è¿›              |
| Volta (sm_70)   | Tensor Coreã€ç‹¬ç«‹çº¿ç¨‹è°ƒåº¦ |
| Ampere (sm_80)  | å¼‚æ­¥æ‹·è´ã€æ›´å¤§ L2         |
| Hopper (sm_90)  | Transformer Engine        |

### ç¼–å†™å¯ç§»æ¤ä»£ç 

**å‚æ•°åŒ–å…³é”®å¸¸é‡**ï¼š

```cuda
#if __CUDA_ARCH__ >= 800
    #define SHARED_MEM_SIZE 164 * 1024  // Ampere: 164KB
#elif __CUDA_ARCH__ >= 700
    #define SHARED_MEM_SIZE 96 * 1024   // Volta: 96KB
#else
    #define SHARED_MEM_SIZE 48 * 1024   // é»˜è®¤: 48KB
#endif
```

**è¿è¡Œæ—¶æŸ¥è¯¢èƒ½åŠ›**ï¼š

```cuda
cudaDeviceProp prop;
cudaGetDeviceProperties(&prop, 0);

int blocks_per_sm = prop.maxBlocksPerMultiProcessor;
int shared_mem = prop.sharedMemPerBlock;
int max_threads = prop.maxThreadsPerBlock;
```

### è·¨å¹³å°æŠ€æœ¯

| æŠ€æœ¯   | æè¿°          | ä¼˜åŠ¿          |
| ------ | ------------- | ------------- |
| CUDA   | NVIDIA ä¸“ç”¨   | æ€§èƒ½æœ€ä½³      |
| OpenCL | è·¨å‚å•†æ ‡å‡†    | å¯ç§»æ¤æ€§      |
| SYCL   | C++ æ ‡å‡†åŒ–    | ç°ä»£ C++ é£æ ¼ |
| HIP    | AMD å…¼å®¹ CUDA | æ˜“äºè¿ç§»      |
| Kokkos | æŠ½è±¡å±‚        | å¤šåç«¯æ”¯æŒ    |

## è°ƒè¯•ä¸éªŒè¯

### å¸¸è§é”™è¯¯ç±»å‹

1. **å†…å­˜é”™è¯¯**ï¼šè¶Šç•Œè®¿é—®ã€æœªåˆå§‹åŒ–å†…å­˜
2. **ç«æ€æ¡ä»¶**ï¼šåŒæ­¥ä¸å½“å¯¼è‡´æ•°æ®ç«äº‰
3. **æ•°å€¼è¯¯å·®**ï¼šæµ®ç‚¹ç²¾åº¦ã€èˆå…¥è¯¯å·®
4. **æ­»é”**ï¼šåŒæ­¥åŸè¯­ä½¿ç”¨ä¸å½“

### è°ƒè¯•å·¥å…·

| å·¥å…·                 | ç”¨é€”             |
| -------------------- | ---------------- |
| cuda-memcheck        | å†…å­˜é”™è¯¯æ£€æµ‹     |
| compute-sanitizer    | æ–°ä¸€ä»£é”™è¯¯æ£€æµ‹   |
| Nsight Eclipse       | IDE é›†æˆè°ƒè¯•     |
| Nsight Visual Studio | Windows è°ƒè¯•     |
| printf               | ç®€å•è°ƒè¯•ï¼ˆæ…ç”¨ï¼‰ |

### éªŒè¯ç­–ç•¥

```cuda
// 1. ä¿ç•™ CPU å‚è€ƒå®ç°
void cpu_reference(float *output, float *input, int n);

// 2. å¯¹æ¯” GPU ç»“æœ
bool verify_result(float *gpu, float *cpu, int n, float epsilon) {
    for (int i = 0; i < n; i++) {
        if (fabsf(gpu[i] - cpu[i]) > epsilon) {
            printf("Mismatch at %d: GPU=%.6f, CPU=%.6f\n", 
                   i, gpu[i], cpu[i]);
            return false;
        }
    }
    return true;
}

// 3. æ¸è¿›å¼æµ‹è¯•
// å°è§„æ¨¡ â†’ ä¸­è§„æ¨¡ â†’ å¤§è§„æ¨¡
// 1 Block â†’ å¤š Block â†’ å¤š SM
```

## ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### é”™è¯¯å¤„ç†

```cuda
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA error at %s:%d: %s\n", \
                    __FILE__, __LINE__, cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

// ä½¿ç”¨
CUDA_CHECK(cudaMalloc(&d_ptr, size));
CUDA_CHECK(cudaMemcpy(d_ptr, h_ptr, size, cudaMemcpyHostToDevice));
```

### èµ„æºç®¡ç†

```cuda
// RAII é£æ ¼ç®¡ç† CUDA èµ„æº
class CudaBuffer {
    void *ptr;
    size_t size;
public:
    CudaBuffer(size_t n) : size(n) {
        CUDA_CHECK(cudaMalloc(&ptr, n));
    }
    ~CudaBuffer() {
        cudaFree(ptr);
    }
    void* get() { return ptr; }
    // ç¦æ­¢æ‹·è´
    CudaBuffer(const CudaBuffer&) = delete;
    CudaBuffer& operator=(const CudaBuffer&) = delete;
};
```

### æ€§èƒ½ç›‘æ§

```cuda
// è®¡æ—¶
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start);
my_kernel<<<grid, block>>>(args);
cudaEventRecord(stop);

cudaEventSynchronize(stop);
float milliseconds = 0;
cudaEventElapsedTime(&milliseconds, start, stop);

printf("Kernel time: %.3f ms\n", milliseconds);

cudaEventDestroy(start);
cudaEventDestroy(stop);
```

## GPU è®¡ç®—ç”Ÿæ€ç³»ç»Ÿ

### CUDA åº“

| åº“       | é¢†åŸŸ     | åŠŸèƒ½         |
| -------- | -------- | ------------ |
| cuBLAS   | çº¿æ€§ä»£æ•° | çŸ©é˜µè¿ç®—     |
| cuFFT    | ä¿¡å·å¤„ç† | å‚…é‡Œå¶å˜æ¢   |
| cuDNN    | æ·±åº¦å­¦ä¹  | ç¥ç»ç½‘ç»œåŸè¯­ |
| cuSPARSE | ç¨€ç–çŸ©é˜µ | ç¨€ç–è¿ç®—     |
| cuRAND   | éšæœºæ•°   | ä¼ªéšæœºç”Ÿæˆ   |
| Thrust   | é€šç”¨     | STL é£æ ¼å®¹å™¨ |
| CUB      | åº•å±‚     | åŸè¯­åº“       |

### ä½¿ç”¨åº“çš„ä¼˜åŠ¿

1. **ç»è¿‡é«˜åº¦ä¼˜åŒ–**ï¼šä¸“å®¶å›¢é˜ŸæŒç»­è°ƒä¼˜
2. **ç‰ˆæœ¬é—´æ”¹è¿›**ï¼šè‡ªåŠ¨å—ç›Šäºæ–°æ¶æ„ä¼˜åŒ–
3. **å‡å°‘å¼€å‘æ—¶é—´**ï¼šä¸“æ³¨ä¸šåŠ¡é€»è¾‘
4. **å‡å°‘é”™è¯¯**ï¼šç»è¿‡å¹¿æ³›æµ‹è¯•

### ä½•æ—¶è‡ªå·±å®ç°

- ç‰¹æ®Šéœ€æ±‚åº“ä¸æ»¡è¶³
- åº“çš„é€šç”¨å®ç°ä¸å¤Ÿé«˜æ•ˆ
- å­¦ä¹ ç›®çš„

## æœªæ¥è¶‹åŠ¿

### ç¡¬ä»¶æ¼”è¿›

**æ›´å¤šæ ¸å¿ƒ**ï¼š

- SM æ•°é‡æŒç»­å¢åŠ 
- æ›´é«˜çš„å¹¶è¡Œåº¦
- å¯¹è´Ÿè½½å‡è¡¡è¦æ±‚æ›´é«˜

**ä¸“ç”¨åŠ é€Ÿå™¨**ï¼š

- Tensor Coreï¼ˆçŸ©é˜µè¿ç®—ï¼‰
- RT Coreï¼ˆå…‰çº¿è¿½è¸ªï¼‰
- Transformer Engineï¼ˆAIï¼‰

**å†…å­˜æŠ€æœ¯**ï¼š

- HBM å¸¦å®½æŒç»­å¢é•¿
- æ›´å¤§çš„ L2 ç¼“å­˜
- ç»Ÿä¸€å†…å­˜æ€§èƒ½æ”¹è¿›

### ç¼–ç¨‹æ¨¡å‹æ¼”è¿›

**æ›´é«˜æŠ½è±¡**ï¼š

- CUDA Graphsï¼šé™æ€ä»»åŠ¡å›¾
- CUDA Cooperative Groupsï¼šçµæ´»åŒæ­¥
- C++ æ ‡å‡†å¹¶è¡Œï¼šstd::execution

**è·¨å¹³å°æ ‡å‡†**ï¼š

- SYCL æ™®åŠ
- oneAPI ç”Ÿæ€
- å¯ç§»æ¤æ€§é‡è¦æ€§å¢åŠ 

### åº”ç”¨é¢†åŸŸæ‰©å±•

**ä¼ ç»Ÿ HPC**ï¼š

- æ°”è±¡æ¨¡æ‹Ÿ
- åˆ†å­åŠ¨åŠ›å­¦
- æµä½“åŠ›å­¦

**AI/ML**ï¼š

- å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒ
- æ¨ç†åŠ é€Ÿ
- è‡ªåŠ¨é©¾é©¶

**æ–°å…´é¢†åŸŸ**ï¼š

- é‡å­è®¡ç®—æ¨¡æ‹Ÿ
- æ•°å­—å­ªç”Ÿ
- å…ƒå®‡å®™æ¸²æŸ“

## æŒç»­å­¦ä¹ èµ„æº

### å®˜æ–¹èµ„æº

- **CUDA æ–‡æ¡£**ï¼š<https://docs.nvidia.com/cuda/>
- **NVIDIA Developer Blog**ï¼šæŠ€æœ¯æ–‡ç« å’Œæœ€ä½³å®è·µ
- **GTC å¤§ä¼š**ï¼šæœ€æ–°æŠ€æœ¯å‘å¸ƒ
- **CUDA Zone**ï¼šç¤ºä¾‹ä»£ç å’Œæ•™ç¨‹

### å­¦æœ¯èµ„æº

- **PMPP æ•™æ**ï¼šæœ¬ä¹¦æ˜¯ç»å…¸å‚è€ƒ
- **UIUC ECE 408**ï¼šé…å¥—è¯¾ç¨‹ï¼ˆCoursera å¯çœ‹ï¼‰
- **Stanford CS 149**ï¼šå¹¶è¡Œè®¡ç®—åŸºç¡€
- **è®ºæ–‡**ï¼šGTCã€SCã€ICS ä¼šè®®

### ç¤¾åŒº

- **Stack Overflow**ï¼šcuda æ ‡ç­¾
- **NVIDIA å¼€å‘è€…è®ºå›**ï¼šå®˜æ–¹æ”¯æŒ
- **GitHub**ï¼šå¼€æº CUDA é¡¹ç›®
- **Reddit r/CUDA**ï¼šè®¨è®ºç¤¾åŒº

## å…¨ä¹¦å›é¡¾

### åŸºç¡€ç¯‡ï¼ˆ1-6ç« ï¼‰

| ç« èŠ‚ | ä¸»é¢˜     | æ ¸å¿ƒæ¦‚å¿µ             |
| ---- | -------- | -------------------- |
| 1    | å¼•è¨€     | å¼‚æ„è®¡ç®—ã€CUDA ç”Ÿæ€  |
| 2    | æ•°æ®å¹¶è¡Œ | Threadã€Blockã€Grid  |
| 3    | å¤šç»´æ•°æ® | çº¿ç¨‹ç´¢å¼•ã€è¾¹ç•Œæ£€æŸ¥   |
| 4    | è®¡ç®—æ¶æ„ | SMã€Warpã€è°ƒåº¦       |
| 5    | å†…å­˜æ¶æ„ | å±‚æ¬¡ç»“æ„ã€Tiling     |
| 6    | æ€§èƒ½è€ƒè™‘ | åˆå¹¶è®¿é—®ã€å‘æ•£ã€èµ„æº |

### æ¨¡å¼ç¯‡ï¼ˆ7-15ç« ï¼‰

| ç« èŠ‚ | ä¸»é¢˜     | æ ¸å¿ƒæ¨¡å¼              |
| ---- | -------- | --------------------- |
| 7    | å·ç§¯     | Tilingã€Halo          |
| 8    | æ¨¡æ¿     | ç¼“å­˜ã€Register Tiling |
| 9    | ç›´æ–¹å›¾   | åŸå­æ“ä½œã€ç§æœ‰åŒ–      |
| 10   | å½’çº¦     | æ ‘å½¢å½’çº¦ã€Warp åŸè¯­   |
| 11   | å‰ç¼€å’Œ   | Scanã€Work-Efficient  |
| 12   | å½’å¹¶     | Co-Rankã€å¾ªç¯å±•å¼€     |
| 13   | æ’åº     | åŸºæ•°æ’åºã€å¹¶è¡Œå½’å¹¶    |
| 14   | ç¨€ç–çŸ©é˜µ | CSR/ELL/COO æ ¼å¼      |
| 15   | å›¾ç®—æ³•   | BFSã€è¾¹ç•Œæ¨è¿›         |

### åº”ç”¨ç¯‡ï¼ˆ16-18ç« ï¼‰

| ç« èŠ‚ | ä¸»é¢˜     | åº”ç”¨é¢†åŸŸ           |
| ---- | -------- | ------------------ |
| 16   | æ·±åº¦å­¦ä¹  | å·ç§¯ã€æ± åŒ–ã€å…¨è¿æ¥ |
| 17   | MRI é‡å»º | NUFFTã€å…±è½­æ¢¯åº¦    |
| 18   | é™ç”µåŠ¿èƒ½ | N-bodyã€ç©ºé—´åˆ†åŒº   |

### é«˜çº§ç¯‡ï¼ˆ19-22ç« ï¼‰

| ç« èŠ‚ | ä¸»é¢˜     | æ ¸å¿ƒå†…å®¹           |
| ---- | -------- | ------------------ |
| 19   | è®¡ç®—æ€ç»´ | æ–¹æ³•è®ºã€è®¾è®¡åŸåˆ™   |
| 20   | é›†ç¾¤ç¼–ç¨‹ | MPI+CUDAã€Haloäº¤æ¢ |
| 21   | åŠ¨æ€å¹¶è¡Œ | è®¾å¤‡ç«¯å¯åŠ¨ã€é€’å½’   |
| 22   | é«˜çº§å®è·µ | æœ€ä½³å®è·µã€æœªæ¥è¶‹åŠ¿ |

## å†™åœ¨æœ€å

### å¹¶è¡Œç¼–ç¨‹å¿ƒæ³•åæ¡

1. **ç†è§£ç¡¬ä»¶**ï¼šäº†è§£ GPU æ¶æ„ï¼Œæ‰¬é•¿é¿çŸ­
2. **æ•°æ®ä¸ºç‹**ï¼šæ€§èƒ½é€šå¸¸å—é™äºæ•°æ®ç§»åŠ¨
3. **æœ€å¤§åŒ–å¹¶è¡Œ**ï¼šæš´éœ²è¶³å¤Ÿçš„å¹¶è¡Œæ€§éšè—å»¶è¿Ÿ
4. **æœ€å°åŒ–åŒæ­¥**ï¼šåŒæ­¥æ˜¯æ€§èƒ½æ€æ‰‹
5. **åˆå¹¶è®¿é—®**ï¼šè®©å†…å­˜è®¿é—®è¿ç»­
6. **å¤ç”¨æ•°æ®**ï¼šå…±äº«å†…å­˜æ˜¯ä½ æœ€å¥½çš„æœ‹å‹
7. **é¿å…å‘æ•£**ï¼šè®© Warp å†…çº¿ç¨‹èµ°ç›¸åŒè·¯å¾„
8. **æƒè¡¡å–èˆ**ï¼šæ²¡æœ‰é“¶å¼¹ï¼Œåªæœ‰é€‚åˆçš„è§£
9. **Profile ä¼˜å…ˆ**ï¼šæ•°æ®é©±åŠ¨ä¼˜åŒ–ï¼Œä¸è¦çŒœæµ‹
10. **æ¸è¿›è¿­ä»£**ï¼šå…ˆæ­£ç¡®ï¼Œåä¼˜åŒ–ï¼ŒæŒç»­æ”¹è¿›

### ä»å­¦ä¹ åˆ°å®è·µ

```
é˜¶æ®µä¸€ï¼šç†è§£åŸºç¡€
  â†“ å¤šå†™ä»£ç ã€å¤šåšç»ƒä¹ 
é˜¶æ®µäºŒï¼šæŒæ¡æ¨¡å¼
  â†“ åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨
é˜¶æ®µä¸‰ï¼šå½¢æˆç›´è§‰
  â†“ é˜…è¯»è®ºæ–‡ã€å‚ä¸å¼€æº
é˜¶æ®µå››ï¼šæŒç»­ç²¾è¿›
```

### ç»“è¯­

è¿™æœ¬ä¹¦å¸¦ä½ ä» CUDA å…¥é—¨èµ°åˆ°äº†é«˜çº§å®è·µã€‚ä½†å­¦ä¹ æ°¸æ— æ­¢å¢ƒâ€”â€”GPU æŠ€æœ¯å¿«é€Ÿæ¼”è¿›ï¼Œæ–°æ¶æ„ã€æ–°ç‰¹æ€§ä¸æ–­æ¶Œç°ã€‚

**æ ¸å¿ƒèƒ½åŠ›**èƒœè¿‡**å…·ä½“çŸ¥è¯†**ï¼š

- ç†è§£å¹¶è¡Œè®¡ç®—åŸç† > è®°ä½ API ç»†èŠ‚
- æŒæ¡ä¼˜åŒ–æ–¹æ³•è®º > èƒŒè¯µä¼˜åŒ–æŠ€å·§
- åŸ¹å…»è®¡ç®—æ€ç»´ > å¤åˆ¶ä»£ç æ¨¡æ¿

å¸Œæœ›è¿™æœ¬ä¹¦å’Œè¿™ä¸ªç³»åˆ—åšå®¢èƒ½å¸®åŠ©ä½ å»ºç«‹æ‰å®çš„å¹¶è¡Œè®¡ç®—åŸºç¡€ã€‚æ¥ä¸‹æ¥ï¼Œä¸æ–­å®è·µã€æŒç»­å­¦ä¹ ï¼Œåœ¨ GPU è®¡ç®—çš„ä¸–ç•Œé‡Œæ¢ç´¢æ›´å¤šå¯èƒ½ï¼

---

**å‚è€ƒèµ„æ–™ï¼š**

- Hwu, W., Kirk, D., & El Hajj, I. (2022). *Programming Massively Parallel Processors: A Hands-on Approach* (4th Edition). Morgan Kaufmann.
- NVIDIA. *CUDA C++ Programming Guide*. <https://docs.nvidia.com/cuda/cuda-c-programming-guide/>
- NVIDIA. *CUDA C++ Best Practices Guide*. <https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/>

---

> **æœ¬æ–‡ GitHub ä»“åº“**: [https://github.com/psmarter/PMPP-Learning](https://github.com/psmarter/PMPP-Learning)
>
> **ç³»åˆ—å®Œç»“**ï¼šæ„Ÿè°¢é˜…è¯» PMPP å…¨ä¹¦ 22 ç« åšå®¢ç³»åˆ—ï¼
