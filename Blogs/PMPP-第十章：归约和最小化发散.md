---
title: PMPP-ç¬¬åç« ï¼šå½’çº¦å’Œæœ€å°åŒ–å‘æ•£
tags:
  - CUDA
  - GPUç¼–ç¨‹
  - å¹¶è¡Œè®¡ç®—
  - PMPP
  - å½’çº¦
  - åˆ†æ”¯å‘æ•£
  - å¹¶è¡Œç®—æ³•
categories: çŸ¥è¯†åˆ†äº«
cover: /img/PMPP.jpg
abbrlink: 43b40d12
date: 2026-01-18 10:14:18
---

## å‰è¨€

ç¬¬ä¹ç« å­¦ä¹ äº†ç›´æ–¹å›¾è®¡ç®—ï¼Œä½¿ç”¨åŸå­æ“ä½œå¤„ç†è¾“å‡ºå†²çªã€‚æœ¬ç« å­¦ä¹ **å½’çº¦ï¼ˆReductionï¼‰**â€”â€”æŠŠä¸€ç»„æ•°æ®"å½’çº¦"æˆä¸€ä¸ªå€¼ï¼Œä¾‹å¦‚æ±‚å’Œã€æ±‚æœ€å¤§å€¼ã€‚å½’çº¦æ“ä½œçœ‹ä¼¼ç®€å•ï¼Œå®é™…æ¶‰åŠå¹¶è¡Œç®—æ³•çš„æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•é«˜æ•ˆåœ°åˆå¹¶éƒ¨åˆ†ç»“æœï¼Ÿå¦‚ä½•é¿å…åˆ†æ”¯å‘æ•£ï¼Ÿå¦‚ä½•æœ€å¤§åŒ–ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Ÿç¬¬åç« ç³»ç»Ÿè®²è§£è¿™äº›ä¼˜åŒ–æŠ€æœ¯ã€‚

> **ğŸ“¦ é…å¥—èµ„æº**ï¼šæœ¬ç³»åˆ—æ–‡ç« é…æœ‰å®Œæ•´çš„ [GitHub ä»“åº“](https://github.com/psmarter/PMPP-Learning)ï¼ŒåŒ…å«æ¯ç« çš„ç»ƒä¹ é¢˜è§£ç­”ã€CUDA ä»£ç å®ç°å’Œè¯¦ç»†æ³¨é‡Šã€‚æ‰€æœ‰ä»£ç éƒ½ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥ç›´æ¥è¿è¡Œã€‚

## å½’çº¦åŸºç¡€

### ä»€ä¹ˆæ˜¯å½’çº¦

**å½’çº¦**ï¼šç”¨ä¸€ä¸ª**äºŒå…ƒç»“åˆè¿ç®—**æŠŠ N ä¸ªå…ƒç´ åˆå¹¶æˆ 1 ä¸ªå€¼ã€‚

**å¸¸è§å½’çº¦æ“ä½œ**ï¼š

| æ“ä½œ   | è¿ç®—ç¬¦ | å•ä½å…ƒ | ç¤ºä¾‹                      |
| ------ | ------ | ------ | ------------------------- |
| æ±‚å’Œ   | +      | 0      | 1+2+3+4 = 10              |
| æ±‚ç§¯   | Ã—      | 1      | 1Ã—2Ã—3Ã—4 = 24              |
| æœ€å¤§å€¼ | max    | -âˆ     | max(1,5,3,2) = 5          |
| æœ€å°å€¼ | min    | +âˆ     | min(1,5,3,2) = 1          |
| é€»è¾‘ä¸ | &&     | true   | true && false = false     |
| é€»è¾‘æˆ– | \|\|   | false  | true \|\| false = true    |
| ä½ä¸   | &      | ~0     | 0b1100 & 0b1010 = 0b1000  |
| ä½æˆ–   | \|     | 0      | 0b1100 \| 0b1010 = 0b1110 |

**ç»“åˆå¾‹**æ˜¯å…³é”®ï¼š`(a âŠ• b) âŠ• c = a âŠ• (b âŠ• c)`

æœ‰äº†ç»“åˆå¾‹ï¼Œå°±èƒ½ä»»æ„åˆ†ç»„å¹¶è¡Œè®¡ç®—ã€‚

### ä¸²è¡Œå½’çº¦

```c
float sum_sequential(float *data, int n) {
    float sum = 0;
    for (int i = 0; i < n; i++) {
        sum += data[i];
    }
    return sum;
}
```

æ—¶é—´å¤æ‚åº¦ O(n)ï¼Œæ— æ³•åˆ©ç”¨å¹¶è¡Œæ€§ã€‚

### å¹¶è¡Œå½’çº¦çš„æ€è·¯

**æ ‘å½¢å½’çº¦**ï¼š

```
Level 0: [a0, a1, a2, a3, a4, a5, a6, a7]
Level 1: [a0+a1, a2+a3, a4+a5, a6+a7]
Level 2: [a0+a1+a2+a3, a4+a5+a6+a7]
Level 3: [a0+a1+a2+a3+a4+a5+a6+a7]
```

æ¯å±‚æŠŠå…ƒç´ æ•°å‡åŠï¼Œlogâ‚‚N å±‚åå¾—åˆ°ç»“æœã€‚

**æ—¶é—´å¤æ‚åº¦**ï¼šO(log N) æ­¥ï¼Œæ¯æ­¥ O(N/2^k) æ¬¡æ“ä½œ

**å·¥ä½œé‡**ï¼šæ€»æ“ä½œæ•° = N/2 + N/4 + ... + 1 = N - 1ï¼ˆä¸ä¸²è¡Œç›¸åŒï¼‰

**å¹¶è¡Œåº¦**ï¼šç¬¬ k å±‚éœ€è¦ N/2^k ä¸ªå¹¶è¡Œæ“ä½œ

## æœ´ç´ å¹¶è¡Œå½’çº¦

### ç›¸é‚»é…å¯¹

æœ€ç›´è§‚çš„å¹¶è¡ŒåŒ–ï¼šç›¸é‚»å…ƒç´ é…å¯¹æ±‚å’Œ

```cuda
__global__ void reduce_naive(float *g_data, float *g_result, int n) {
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    // åŠ è½½åˆ°å…±äº«å†…å­˜
    sdata[tid] = (i < n) ? g_data[i] : 0;
    __syncthreads();
    
    // æ ‘å½¢å½’çº¦
    for (int stride = 1; stride < blockDim.x; stride *= 2) {
        if (tid % (2 * stride) == 0) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }
    
    // å†™å›ç»“æœ
    if (tid == 0) {
        g_result[blockIdx.x] = sdata[0];
    }
}
```

### é—®é¢˜ï¼šåˆ†æ”¯å‘æ•£

çœ‹è¿™ä¸ªæ¡ä»¶ï¼š`if (tid % (2 * stride) == 0)`

**Level 0** (stride=1)ï¼šçº¿ç¨‹ 0,2,4,6... æ´»è·ƒï¼Œ1,3,5,7... ç©ºé—²
**Level 1** (stride=2)ï¼šçº¿ç¨‹ 0,4,8... æ´»è·ƒ
**Level 2** (stride=4)ï¼šçº¿ç¨‹ 0,8... æ´»è·ƒ

Warp å†…æœ‰çš„çº¿ç¨‹æ´»è·ƒã€æœ‰çš„ç©ºé—² = **åˆ†æ”¯å‘æ•£**ï¼

Warp å¿…é¡»ä¸²è¡Œæ‰§è¡Œä¸¤ä¸ªåˆ†æ”¯ï¼Œæ•ˆç‡å‡åŠã€‚

### é—®é¢˜ï¼šBank å†²çª

`sdata[tid]` å’Œ `sdata[tid + stride]` çš„è®¿é—®æ¨¡å¼ï¼š

- stride=1ï¼šçº¿ç¨‹ 0 è®¿é—® [0,1]ï¼Œçº¿ç¨‹ 2 è®¿é—® [2,3]...ï¼ˆæ— å†²çªï¼‰
- stride=16ï¼šçº¿ç¨‹ 0 è®¿é—® [0,16]ï¼Œçº¿ç¨‹ 32 è®¿é—® [32,48]...

stride æ˜¯ 2 çš„å¹‚æ—¶ï¼Œå¯èƒ½äº§ç”Ÿ Bank å†²çªã€‚

## ä¼˜åŒ– 1ï¼šäº¤é”™é…å¯¹

### æ”¹è¿›æ€è·¯

æŠŠ"ç©ºé—²çº¿ç¨‹åœ¨å³è¾¹"æ”¹æˆ"ç©ºé—²çº¿ç¨‹åœ¨åé¢"ï¼š

```
æœ´ç´ ï¼šçº¿ç¨‹ 0,2,4,6 æ´»è·ƒï¼Œ1,3,5,7 ç©ºé—²
äº¤é”™ï¼šçº¿ç¨‹ 0,1,2,3 æ´»è·ƒï¼Œ4,5,6,7 ç©ºé—²
```

è¿™æ ·ï¼Œæ´»è·ƒçº¿ç¨‹æ˜¯è¿ç»­çš„ï¼Œå‰å‡ ä¸ª Warp æ»¡è½½ï¼Œæœ€åçš„ Warp æ‰é€æ¸ç©ºé—²ã€‚

### å®ç°

```cuda
__global__ void reduce_interleaved(float *g_data, float *g_result, int n) {
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    sdata[tid] = (i < n) ? g_data[i] : 0;
    __syncthreads();
    
    // äº¤é”™å½’çº¦
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        g_result[blockIdx.x] = sdata[0];
    }
}
```

### åˆ†æ

**Level 0** (stride=128)ï¼šçº¿ç¨‹ 0-127 æ´»è·ƒ
**Level 1** (stride=64)ï¼šçº¿ç¨‹ 0-63 æ´»è·ƒ
**Level 2** (stride=32)ï¼šçº¿ç¨‹ 0-31 æ´»è·ƒï¼ˆæ°å¥½ä¸€ä¸ª Warpï¼‰

å‰å‡ å±‚æœ‰å¤šä¸ªå®Œæ•´ Warp åŒæ—¶å·¥ä½œï¼Œåˆ†æ”¯å‘æ•£åªåœ¨æœ€åå‡ å±‚å‡ºç°ã€‚

**æ€§èƒ½æå‡**ï¼šçº¦ 2Ã— ç›¸æ¯”æœ´ç´ ç‰ˆæœ¬ã€‚

## ä¼˜åŒ– 2ï¼šé¦–æ¬¡åŠ è½½æ—¶å½’çº¦

### è§‚å¯Ÿ

æ¯ä¸ªçº¿ç¨‹åªåŠ è½½ä¸€ä¸ªå…ƒç´ ï¼Œä½† Block æ•°å¯èƒ½è¿œå¤šäº SM æ•°ã€‚å¦‚æœè®©æ¯ä¸ªçº¿ç¨‹åŠ è½½å¤šä¸ªå…ƒç´ å¹¶åœ¨åŠ è½½æ—¶æ±‚å’Œï¼Œå¯ä»¥å‡å°‘ Block æ•°ï¼Œæé«˜æ•ˆç‡ã€‚

### å®ç°

```cuda
__global__ void reduce_first_add(float *g_data, float *g_result, int n) {
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int i = blockIdx.x * (blockDim.x * 2) + threadIdx.x;
    
    // é¦–æ¬¡åŠ è½½æ—¶å°±æ±‚å’Œä¸¤ä¸ªå…ƒç´ 
    float sum = 0;
    if (i < n) sum += g_data[i];
    if (i + blockDim.x < n) sum += g_data[i + blockDim.x];
    sdata[tid] = sum;
    __syncthreads();
    
    // åç»­å½’çº¦
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        g_result[blockIdx.x] = sdata[0];
    }
}
```

### æ‰©å±•ï¼šGrid-Stride åŠ è½½

è®©æ¯ä¸ªçº¿ç¨‹åŠ è½½å¤šä¸ªå…ƒç´ ï¼š

```cuda
__global__ void reduce_grid_stride(float *g_data, float *g_result, int n) {
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int gridSize = blockDim.x * gridDim.x;
    
    // Grid-stride ç´¯åŠ 
    float sum = 0;
    while (i < n) {
        sum += g_data[i];
        i += gridSize;
    }
    sdata[tid] = sum;
    __syncthreads();
    
    // æ ‘å½¢å½’çº¦
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        g_result[blockIdx.x] = sdata[0];
    }
}
```

**ä¼˜åŠ¿**ï¼š

1. Block æ•°é‡å¯ä»¥å›ºå®šï¼ˆå¦‚ 256ï¼‰ï¼Œä¸éšæ•°æ®é‡å˜åŒ–
2. æ¯çº¿ç¨‹åšæ›´å¤šæœ‰æ•ˆå·¥ä½œï¼Œåˆ†æ‘ŠåŒæ­¥å¼€é”€
3. æ›´å¥½çš„æŒ‡ä»¤çº§å¹¶è¡Œ

## ä¼˜åŒ– 3ï¼šå±•å¼€æœ€å Warp

### è§‚å¯Ÿ

å½“ stride < 32 æ—¶ï¼Œåªæœ‰ä¸€ä¸ª Warp åœ¨å·¥ä½œã€‚Warp å†…çº¿ç¨‹è‡ªåŠ¨åŒæ­¥ï¼ˆSIMTï¼‰ï¼Œä¸éœ€è¦ `__syncthreads()`ï¼

### å®ç°

```cuda
__device__ void warpReduce(volatile float *sdata, int tid) {
    sdata[tid] += sdata[tid + 32];
    sdata[tid] += sdata[tid + 16];
    sdata[tid] += sdata[tid + 8];
    sdata[tid] += sdata[tid + 4];
    sdata[tid] += sdata[tid + 2];
    sdata[tid] += sdata[tid + 1];
}

__global__ void reduce_warp_unroll(float *g_data, float *g_result, int n) {
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x * 2 + threadIdx.x;
    
    sdata[tid] = 0;
    if (i < n) sdata[tid] += g_data[i];
    if (i + blockDim.x < n) sdata[tid] += g_data[i + blockDim.x];
    __syncthreads();
    
    // å¸¸è§„å½’çº¦ï¼ˆstride >= 64ï¼‰
    for (int stride = blockDim.x / 2; stride > 32; stride >>= 1) {
        if (tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }
    
    // Warp å†…å±•å¼€ï¼ˆstride < 32ï¼‰
    if (tid < 32) {
        warpReduce(sdata, tid);
    }
    
    if (tid == 0) {
        g_result[blockIdx.x] = sdata[0];
    }
}
```

### å…³é”®ç‚¹

**volatile**ï¼šå‘Šè¯‰ç¼–è¯‘å™¨ä¸è¦ä¼˜åŒ–æ‰å¯¹ sdata çš„è¯»å†™ï¼Œç¡®ä¿æ¯æ¬¡éƒ½è®¿é—®å…±äº«å†…å­˜ã€‚

**éšå¼åŒæ­¥**ï¼šWarp å†…çš„ 32 ä¸ªçº¿ç¨‹æ‰§è¡Œç›¸åŒæŒ‡ä»¤ï¼Œè‡ªåŠ¨ä¿æŒåŒæ­¥ã€‚

**æ€§èƒ½æå‡**ï¼šå‡å°‘ 5 æ¬¡ `__syncthreads()` è°ƒç”¨ã€‚

## ä¼˜åŒ– 4ï¼šå®Œå…¨å±•å¼€

### å½“ Block å¤§å°å·²çŸ¥æ—¶

å¦‚æœ Block å¤§å°æ˜¯ç¼–è¯‘æ—¶å¸¸é‡ï¼ˆå¦‚ 256ï¼‰ï¼Œå¯ä»¥å®Œå…¨å±•å¼€å¾ªç¯ï¼š

```cuda
template <unsigned int blockSize>
__global__ void reduce_complete_unroll(float *g_data, float *g_result, int n) {
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int i = blockIdx.x * blockSize * 2 + threadIdx.x;
    
    sdata[tid] = 0;
    if (i < n) sdata[tid] += g_data[i];
    if (i + blockSize < n) sdata[tid] += g_data[i + blockSize];
    __syncthreads();
    
    // ç¼–è¯‘æ—¶å±•å¼€
    if (blockSize >= 512) {
        if (tid < 256) sdata[tid] += sdata[tid + 256];
        __syncthreads();
    }
    if (blockSize >= 256) {
        if (tid < 128) sdata[tid] += sdata[tid + 128];
        __syncthreads();
    }
    if (blockSize >= 128) {
        if (tid < 64) sdata[tid] += sdata[tid + 64];
        __syncthreads();
    }
    
    // Warp å†…å±•å¼€
    if (tid < 32) {
        volatile float *smem = sdata;
        if (blockSize >= 64) smem[tid] += smem[tid + 32];
        if (blockSize >= 32) smem[tid] += smem[tid + 16];
        if (blockSize >= 16) smem[tid] += smem[tid + 8];
        if (blockSize >= 8)  smem[tid] += smem[tid + 4];
        if (blockSize >= 4)  smem[tid] += smem[tid + 2];
        if (blockSize >= 2)  smem[tid] += smem[tid + 1];
    }
    
    if (tid == 0) {
        g_result[blockIdx.x] = sdata[0];
    }
}
```

### å¯åŠ¨

```cuda
// æ ¹æ® Block å¤§å°é€‰æ‹©æ¨¡æ¿å®ä¾‹
switch (blockSize) {
    case 512: reduce_complete_unroll<512><<<grid, 512, 512*sizeof(float)>>>(...); break;
    case 256: reduce_complete_unroll<256><<<grid, 256, 256*sizeof(float)>>>(...); break;
    case 128: reduce_complete_unroll<128><<<grid, 128, 128*sizeof(float)>>>(...); break;
    // ...
}
```

**ä¼˜åŠ¿**ï¼šç¼–è¯‘å™¨å¯ä»¥å®Œå…¨å±•å¼€å¾ªç¯ï¼Œæ¶ˆé™¤å¾ªç¯å¼€é”€å’Œåˆ†æ”¯ã€‚

## ä¼˜åŒ– 5ï¼šWarp Shuffle

### ç°ä»£æ–¹æ³•

ä» Kepler æ¶æ„å¼€å§‹ï¼ŒCUDA æä¾› **Warp Shuffle** æŒ‡ä»¤ï¼Œçº¿ç¨‹å¯ä»¥ç›´æ¥äº¤æ¢å¯„å­˜å™¨å€¼ï¼Œæ— éœ€å…±äº«å†…å­˜ï¼š

```cuda
__device__ float warpReduceSum(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    return val;
}
```

### å®Œæ•´å®ç°

```cuda
__global__ void reduce_shuffle(float *g_data, float *g_result, int n) {
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int gridSize = blockDim.x * gridDim.x;
    
    // Grid-stride ç´¯åŠ 
    float sum = 0;
    while (i < n) {
        sum += g_data[i];
        i += gridSize;
    }
    
    // Warp å†…å½’çº¦ï¼ˆshuffleï¼‰
    sum = warpReduceSum(sum);
    
    // Warp é—´å½’çº¦
    __shared__ float warpSums[32];  // æœ€å¤š 32 ä¸ª Warp
    int lane = tid % 32;
    int wid = tid / 32;
    
    if (lane == 0) {
        warpSums[wid] = sum;
    }
    __syncthreads();
    
    // ç¬¬ä¸€ä¸ª Warp åšæœ€ç»ˆå½’çº¦
    if (wid == 0) {
        sum = (tid < blockDim.x / 32) ? warpSums[lane] : 0;
        sum = warpReduceSum(sum);
        
        if (tid == 0) {
            g_result[blockIdx.x] = sum;
        }
    }
}
```

### Shuffle å‡½æ•°

| å‡½æ•°               | åŠŸèƒ½                    |
| ------------------ | ----------------------- |
| `__shfl_sync`      | ä»æŒ‡å®š lane è·å–å€¼      |
| `__shfl_up_sync`   | ä»ä½ lane è·å–å€¼        |
| `__shfl_down_sync` | ä»é«˜ lane è·å–å€¼        |
| `__shfl_xor_sync`  | ä¸ XOR åç§»çš„ lane äº¤æ¢ |

**ä¼˜åŠ¿**ï¼š

1. ä¸éœ€è¦å…±äº«å†…å­˜
2. å»¶è¿Ÿæ¯”å…±äº«å†…å­˜ä½
3. æ—  Bank å†²çª

## å¤šçº§å½’çº¦

### å• Block ä¸å¤Ÿ

å¦‚æœæ•°æ®é‡è¶…è¿‡å• Block èƒ½å¤„ç†çš„èŒƒå›´ï¼Œéœ€è¦å¤šçº§å½’çº¦ï¼š

```
Level 1: æ¯ä¸ª Block å½’çº¦è‡ªå·±çš„éƒ¨åˆ† â†’ gridDim ä¸ªéƒ¨åˆ†å’Œ
Level 2: å†å¯åŠ¨ä¸€ä¸ª Kernel å½’çº¦è¿™äº›éƒ¨åˆ†å’Œ
...
é‡å¤ç›´åˆ°åªå‰©ä¸€ä¸ªå€¼
```

### å®ç°

```cuda
void reduce_multi_level(float *d_data, float *d_result, int n) {
    int blockSize = 256;
    int numBlocks = (n + blockSize * 2 - 1) / (blockSize * 2);
    
    float *d_partial;
    cudaMalloc(&d_partial, numBlocks * sizeof(float));
    
    // Level 1
    reduce_kernel<<<numBlocks, blockSize, blockSize * sizeof(float)>>>
        (d_data, d_partial, n);
    
    // åç»­ Level
    while (numBlocks > 1) {
        int n_next = numBlocks;
        numBlocks = (n_next + blockSize * 2 - 1) / (blockSize * 2);
        
        reduce_kernel<<<numBlocks, blockSize, blockSize * sizeof(float)>>>
            (d_partial, d_partial, n_next);
    }
    
    // æ‹·è´æœ€ç»ˆç»“æœ
    cudaMemcpy(d_result, d_partial, sizeof(float), cudaMemcpyDeviceToHost);
    cudaFree(d_partial);
}
```

### ä¼˜åŒ–ï¼šåŸå­ç´¯åŠ 

å¦‚æœåªéœ€è¦æœ€ç»ˆå’Œï¼Œå¯ä»¥ç”¨åŸå­æ“ä½œé¿å…å¤šçº§ï¼š

```cuda
__global__ void reduce_atomic(float *g_data, float *g_result, int n) {
    // ... å¸¸è§„ Block å†…å½’çº¦ ...
    
    if (tid == 0) {
        atomicAdd(g_result, sdata[0]);  // ç›´æ¥ç´¯åŠ åˆ°ç»“æœ
    }
}
```

**æ³¨æ„**ï¼šéœ€è¦å…ˆå°† `g_result` åˆå§‹åŒ–ä¸º 0ã€‚

## åˆ†æ”¯å‘æ•£æ·±å…¥åˆ†æ

### å‘æ•£çš„ä»£ä»·

å½“ Warp å†…çº¿ç¨‹èµ°ä¸åŒåˆ†æ”¯æ—¶ï¼š

```cuda
if (condition) {
    doA();  // éƒ¨åˆ†çº¿ç¨‹
} else {
    doB();  // å…¶ä»–çº¿ç¨‹
}
```

ç¡¬ä»¶æ‰§è¡Œï¼š

1. æ‰€æœ‰çº¿ç¨‹æ‰§è¡Œ doA()ï¼Œä¸æ»¡è¶³æ¡ä»¶çš„çº¿ç¨‹ç»“æœè¢«ä¸¢å¼ƒ
2. æ‰€æœ‰çº¿ç¨‹æ‰§è¡Œ doB()ï¼Œæ»¡è¶³æ¡ä»¶çš„çº¿ç¨‹ç»“æœè¢«ä¸¢å¼ƒ
3. æ€»æ—¶é—´ = doA() + doB()

**å‘æ•£ç¨‹åº¦**å½±å“æ€§èƒ½ï¼š

| å‘æ•£æ¯”ä¾‹     | æ€§èƒ½å½±å“    |
| ------------ | ----------- |
| 0/32         | æ— å½±å“      |
| 1/32         | å‡ ä¹æ— å½±å“  |
| 16/32        | çº¦ 50% æ€§èƒ½ |
| æŒ‰ Warp è¾¹ç•Œ | æ— å‘æ•£      |

### æœ€å°åŒ–å‘æ•£çš„ç­–ç•¥

**1. è®©æ¡ä»¶æŒ‰ Warp å¯¹é½**

```cuda
// å·®ï¼šæ··åˆå‘æ•£
if (tid % 2 == 0) { ... }

// å¥½ï¼šWarp å†…æ— å‘æ•£
if (tid < 128) { ... }  // å‰ 4 ä¸ª Warp vs å 4 ä¸ª Warp
```

**2. ç”¨ç®—æœ¯æ›¿ä»£åˆ†æ”¯**

```cuda
// æœ‰åˆ†æ”¯
if (a > b) result = a;
else result = b;

// æ— åˆ†æ”¯
result = (a > b) * a + (a <= b) * b;

// æ›´å¥½ï¼šç”¨å†…ç½®å‡½æ•°
result = max(a, b);
```

**3. é¢„å…ˆåˆ†ç¦»æ•°æ®**

```cuda
// å·®ï¼šè¿è¡Œæ—¶åˆ†æ”¯
if (data[i] > threshold) processA();
else processB();

// å¥½ï¼šé¢„å¤„ç†åˆ†ç¦»
// Kernel 1: åˆ†ç¦»æ•°æ®
// Kernel 2: æ‰¹é‡å¤„ç† A ç±»
// Kernel 3: æ‰¹é‡å¤„ç† B ç±»
```

### å½’çº¦ä¸­çš„å‘æ•£æ§åˆ¶

æœ´ç´  vs äº¤é”™çš„å¯¹æ¯”ï¼š

```
æœ´ç´  (stride=1):
  Warp 0: threads 0,2,4,...,30 æ´»è·ƒï¼ˆ16ä¸ªï¼‰
  â†ª æ¯ä¸ª Warp éƒ½æœ‰ 50% å‘æ•£

äº¤é”™ (stride=128):
  Warp 0-3: å…¨éƒ¨æ´»è·ƒ
  Warp 4-7: å…¨éƒ¨ç©ºé—²
  â†ª æ²¡æœ‰ Warp å†…å‘æ•£ï¼
```

## æ€§èƒ½å¯¹æ¯”

ä»¥ 2Â²â´ï¼ˆçº¦1600ä¸‡ï¼‰ä¸ªå•ç²¾åº¦æµ®ç‚¹æ•°æ±‚å’Œä¸ºä¾‹ï¼š

| ä¼˜åŒ–ç‰ˆæœ¬          | å¸¦å®½åˆ©ç”¨ç‡ | ç›¸å¯¹æ€§èƒ½ |
| ----------------- | ---------- | -------- |
| æœ´ç´ ç›¸é‚»é…å¯¹      | 4%         | 1Ã—       |
| äº¤é”™é…å¯¹          | 8%         | 2Ã—       |
| é¦–æ¬¡åŠ è½½å½’çº¦      | 16%        | 4Ã—       |
| å±•å¼€æœ€åçº¿ç¨‹æŸ    | 25%        | 6Ã—       |
| å®Œå…¨å±•å¼€          | 40%        | 10Ã—      |
| + çº¿ç¨‹æŸ Shuffle  | 60%        | 15Ã—      |

**æµ‹è¯•ç¯å¢ƒ**ï¼š
- GPUï¼šNVIDIA RTX 3090ï¼ˆ10496 CUDA æ ¸å¿ƒï¼‰
- æ•°æ®é‡ï¼š16,777,216 ä¸ª floatï¼ˆ64MBï¼‰
- å—å¤§å°ï¼š256çº¿ç¨‹

å¸¦å®½åˆ©ç”¨ç‡è¾¾åˆ°60%å·²ç»å¾ˆé«˜â€”â€”å‰©ä½™å¼€é”€æ¥è‡ªæ ¸å‡½æ•°å¯åŠ¨ã€åŒæ­¥ç­‰ä¸å¯é¿å…çš„å¼€é”€ã€‚

## å…¶ä»–å½’çº¦æ“ä½œ

### æœ€å¤§å€¼/æœ€å°å€¼

```cuda
__device__ float warpReduceMax(float val) {
    for (int offset = 16; offset > 0; offset /= 2) {
        val = max(val, __shfl_down_sync(0xffffffff, val, offset));
    }
    return val;
}
```

### å¸¦ç´¢å¼•çš„æœ€å¤§å€¼

è¿”å›æœ€å¤§å€¼åŠå…¶ä½ç½®ï¼š

```cuda
__device__ void warpReduceArgMax(float &val, int &idx) {
    for (int offset = 16; offset > 0; offset /= 2) {
        float other_val = __shfl_down_sync(0xffffffff, val, offset);
        int other_idx = __shfl_down_sync(0xffffffff, idx, offset);
        if (other_val > val) {
            val = other_val;
            idx = other_idx;
        }
    }
}
```

### ç‚¹ç§¯

ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ = é€å…ƒç´ ä¹˜æ³• + æ±‚å’Œå½’çº¦ï¼š

```cuda
__global__ void dotProduct(float *a, float *b, float *result, int n) {
    extern __shared__ float sdata[];
    
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    float sum = 0;
    while (i < n) {
        sum += a[i] * b[i];
        i += blockDim.x * gridDim.x;
    }
    sdata[tid] = sum;
    __syncthreads();
    
    // å½’çº¦
    // ...
}
```

## CUB åº“

### ä¸ºä»€ä¹ˆç”¨åº“

æ‰‹å†™å½’çº¦å®¹æ˜“å‡ºé”™ï¼Œä¸”éš¾ä»¥è¦†ç›–æ‰€æœ‰ä¼˜åŒ–ã€‚NVIDIA çš„ CUB åº“æä¾›é«˜åº¦ä¼˜åŒ–çš„å®ç°ï¼š

```cuda
#include <cub/cub.cuh>

void reduceWithCub(float *d_data, float *d_result, int n) {
    // ç¡®å®šä¸´æ—¶å­˜å‚¨å¤§å°
    void *d_temp = nullptr;
    size_t temp_bytes = 0;
    cub::DeviceReduce::Sum(d_temp, temp_bytes, d_data, d_result, n);
    
    // åˆ†é…ä¸´æ—¶å­˜å‚¨
    cudaMalloc(&d_temp, temp_bytes);
    
    // æ‰§è¡Œå½’çº¦
    cub::DeviceReduce::Sum(d_temp, temp_bytes, d_data, d_result, n);
    
    cudaFree(d_temp);
}
```

### CUB æä¾›çš„å½’çº¦

| å‡½æ•°                   | åŠŸèƒ½         |
| ---------------------- | ------------ |
| `DeviceReduce::Sum`    | æ±‚å’Œ         |
| `DeviceReduce::Max`    | æœ€å¤§å€¼       |
| `DeviceReduce::Min`    | æœ€å°å€¼       |
| `DeviceReduce::ArgMax` | æœ€å¤§å€¼åŠç´¢å¼• |
| `DeviceReduce::ArgMin` | æœ€å°å€¼åŠç´¢å¼• |
| `DeviceReduce::Reduce` | è‡ªå®šä¹‰æ“ä½œç¬¦ |

### Block çº§å½’çº¦

```cuda
#include <cub/cub.cuh>

__global__ void myKernel(float *data, float *result) {
    typedef cub::BlockReduce<float, 256> BlockReduce;
    __shared__ typename BlockReduce::TempStorage temp_storage;
    
    float val = data[threadIdx.x];
    float sum = BlockReduce(temp_storage).Sum(val);
    
    if (threadIdx.x == 0) {
        result[blockIdx.x] = sum;
    }
}
```

## å°ç»“

ç¬¬åç« ç³»ç»Ÿè®²è§£äº†å¹¶è¡Œå½’çº¦ï¼š

**æ ‘å½¢å½’çº¦**ï¼šlog N æ­¥å®Œæˆï¼Œä½†æœ´ç´ å®ç°æœ‰ä¸¥é‡çš„åˆ†æ”¯å‘æ•£å’Œ Bank å†²çªã€‚

**äº¤é”™é…å¯¹**ï¼šè®©æ´»è·ƒçº¿ç¨‹è¿ç»­ï¼Œæ¶ˆé™¤ Warp å†…å‘æ•£ã€‚è¿™æ˜¯æœ€å…³é”®çš„ä¼˜åŒ–ã€‚

**é¦–æ¬¡åŠ è½½å½’çº¦**ï¼šGrid-stride loop è®©æ¯çº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ ï¼Œå‡å°‘ Block æ•°å’ŒåŒæ­¥å¼€é”€ã€‚

**å±•å¼€ä¼˜åŒ–**ï¼šåˆ©ç”¨ Warp å†…éšå¼åŒæ­¥ï¼Œæ¶ˆé™¤æœ€åå‡ å±‚çš„ `__syncthreads()`ã€‚å®Œå…¨å±•å¼€è¿›ä¸€æ­¥æ¶ˆé™¤å¾ªç¯å¼€é”€ã€‚

**Warp Shuffle**ï¼šç°ä»£ GPU çš„åˆ©å™¨ï¼Œç›´æ¥äº¤æ¢å¯„å­˜å™¨ï¼Œæ— éœ€å…±äº«å†…å­˜ã€‚

**åˆ†æ”¯å‘æ•£**ï¼šæŒ‰ Warp è¾¹ç•Œåˆ’åˆ†æ¡ä»¶ï¼Œç”¨ç®—æœ¯æ›¿ä»£åˆ†æ”¯ï¼Œæ˜¯é€šç”¨çš„å‘æ•£æœ€å°åŒ–æŠ€æœ¯ã€‚

**CUB åº“**ï¼šç”Ÿäº§ç¯å¢ƒä¼˜å…ˆä½¿ç”¨åº“ï¼Œå®ƒé›†æˆäº†æ‰€æœ‰ä¼˜åŒ–ä¸”ç»è¿‡å……åˆ†æµ‹è¯•ã€‚

å½’çº¦æ˜¯å¹¶è¡Œè®¡ç®—çš„åŸºç¡€æ¨¡å¼ï¼Œå·ç§¯çš„è¾¹ç•Œå¤„ç†ã€ç›´æ–¹å›¾çš„æœ€ç»ˆåˆå¹¶ã€ç¥ç»ç½‘ç»œçš„ Softmax éƒ½ç”¨åˆ°ã€‚ä¸‹ä¸€ç« å­¦ä¹ å‰ç¼€å’Œâ€”â€”å¦ä¸€ä¸ªåŸºç¡€ä¸”å¼ºå¤§çš„å¹¶è¡ŒåŸè¯­ã€‚

---

## ğŸš€ ä¸‹ä¸€æ­¥

---

## ğŸ“š å‚è€ƒèµ„æ–™

- PMPP ç¬¬å››ç‰ˆ Chapter 10
- [ç¬¬åç« ï¼šå½’çº¦å’Œæœ€å°åŒ–å‘æ•£](https://smarter.xin/posts/43b40d12/)

**å­¦ä¹ æ„‰å¿«ï¼** ğŸ“

---

> **æœ¬æ–‡ GitHub ä»“åº“**: [https://github.com/psmarter/PMPP-Learning](https://github.com/psmarter/PMPP-Learning)
